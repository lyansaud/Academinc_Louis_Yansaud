[{"authors":[],"categories":null,"content":" I performed a sentiment analysis on each individual cryptocurrency using Twitter API to have a better understanding about the influence of social media on cryptocurrencies.\nlibrary(twitteR) library(dplyr) library(tm) library(wordcloud) library(tidytext) library(tidyverse) library(sqldf) library(ggplot2) library(ggthemes) library(data.table) library(gridExtra)  Built-funcrion for Web Scraping Analysis I built a function to do some web-scraping analysis in order to facilitate the extraction of Twitter data.\nsetup_twitter_oauth(consumer_key, consumer_secret, access_token, access_token_secret) twitter_scraping_data \u0026lt;- function(coin_name){ coin_data = twitteR::searchTwitter(paste0(\u0026quot;#\u0026quot;,coin_name,\u0026quot; -filter:retweets\u0026quot;), lang = \u0026quot;en\u0026quot;, n = 100, since = '2017-08-01', until = as.character(as.Date(Sys.time())), retryOnRateLimit = 1) d = twitteR::twListToDF(coin_data) return(d) }  Loading the dataset from CoinMarket library(coinmarketcapr) all_coins \u0026lt;- get_marketcap_ticker_all() list_coin_names \u0026lt;- as.list(tolower(all_coins$name)) list_coin_names_20 \u0026lt;- list_coin_names[1:20]  Data Cleaning Part 1 datalist = list() for (i in list_coin_names_20) { data = twitter_scraping_data(i) data$coin_name = i datalist[[i]] = data } big_data = do.call(rbind, datalist ) big_data_text \u0026lt;- big_data %\u0026gt;% select(text, coin_name) big_data_text$text \u0026lt;- as.character(big_data_text$text) str(big_data_text) #applying to the big data datalist_v2 \u0026lt;- list() for (i in list_coin_names_20) { data = big_data_text %\u0026gt;% filter(coin_name == i) data$text = stripWhitespace(data$text) data$text = gsub(\u0026quot;[^[:alnum:][:space:]$]\u0026quot;, \u0026quot;\u0026quot;, data$text) data$text = tolower(data$text) data$text = removeWords(data$text, c(stopwords(\u0026quot;english\u0026quot;),'ampamp','retweet','just','comment','amp','bitcoin','btc','xrp','eth','crypto','cryptocurrency', paste0(i), all_coins$symbol)) data$coin_name = i datalist_v2[[i]] = data } big_data_clean = do.call(rbind, datalist_v2 )  Data Cleaning Part 2 datalist_v2 \u0026lt;- list() for ( i in list_coin_names_20 ) { data \u0026lt;- big_data_clean %\u0026gt;% filter(coin_name == i) %\u0026gt;% select(text) data$text \u0026lt;- as.character(data$text) datatweets = VectorSource(data$text) datatweets = VCorpus(datatweets) datatweets_dtm\u0026lt;-DocumentTermMatrix(datatweets) datatweets_m\u0026lt;-as.matrix(datatweets_dtm) datatweets_wf\u0026lt;-colSums(datatweets_m) datatweets_wf\u0026lt;-sort(datatweets_wf,decreasing = TRUE) datatweets_wf$coin \u0026lt;- i datalist_v2[[i]] = datatweets_wf } #More data cleaning data1 \u0026lt;- datalist_v2[[\u0026quot;bitcoin\u0026quot;]] data1 \u0026lt;- unlist(data1) data1 \u0026lt;- as.data.frame(data1) data1 \u0026lt;- cbind(words = rownames(data1), data1) rownames(data1) \u0026lt;- c() data1 \u0026lt;- data1 %\u0026gt;% mutate(words_count = data1) %\u0026gt;% select(words, words_count)  Data Cleaning Part 3 #combining everything now datalist_v3 \u0026lt;- list() for ( i in list_coin_names_20) { data1 \u0026lt;- datalist_v2[[i]] data1 \u0026lt;- unlist(data1) data1 \u0026lt;- as.data.frame(data1) data1 \u0026lt;- cbind(words = rownames(data1), data1) rownames(data1) \u0026lt;- c() data1 \u0026lt;- data1 %\u0026gt;% mutate(words_count = data1) %\u0026gt;% select(words, words_count) data1$coin \u0026lt;- i datalist_v3[[i]] = data1 } big_data_clean_2 = do.call(rbind, datalist_v3 ) big_data_clean_2$words_count \u0026lt;- as.numeric(big_data_clean_2$words_count) big_data_clean_2$words \u0026lt;- as.character(big_data_clean_2$words) write.csv(big_data_clean_2, \u0026quot;big_data_clean_2.csv\u0026quot;)  Data Cleaning Part 4 datalist_v4 \u0026lt;- list() for (i in list_coin_names_20) { data \u0026lt;- big_data_text %\u0026gt;% filter(coin_name == i) %\u0026gt;% select(text) data$text \u0026lt;- as.character(data$text) data \u0026lt;- data$text data \u0026lt;- as.tibble(data) data \u0026lt;- data %\u0026gt;% unnest_tokens(word, value) data$coin \u0026lt;- i datalist_v4[[i]] = data } big_data_clean_4 = do.call(rbind, datalist_v4 ) statement = paste0(\u0026quot; select sentiments.*,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word as SentimentWord from sentiments,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot; where sentiments.word = \u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word \u0026quot;) datalist_v5 \u0026lt;- list() for (i in list_coin_names_20) { DF \u0026lt;- big_data_clean_4 %\u0026gt;% filter(coin == i) %\u0026gt;% select(word) statement = paste0(\u0026quot; select sentiments.*,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word as SentimentWord from sentiments,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot; where sentiments.word = \u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word \u0026quot;) data \u0026lt;- sqldf(statement) data$coin \u0026lt;- i datalist_v5[[i]] = data } big_data_clean_5 = do.call(rbind, datalist_v5 ) data_v5 \u0026lt;- big_data_clean_5[!duplicated(big_data_clean_5), ] write.csv(data_v5, \u0026quot;data_crypto_sentiment.csv\u0026quot;) data_crypto_sentiment \u0026lt;- read.csv('data_crypto_sentiment.csv')  ","date":1532070000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534143600,"objectID":"b90faca0b8c52309503e75f801334405","permalink":"/post/web-scraping-twitter/","publishdate":"2018-07-20T00:00:00-07:00","relpermalink":"/post/web-scraping-twitter/","section":"post","summary":"Here is my web scraping analysis using Twitter API to perform sentiment analysis on cryptocurrency","tags":["Academic"],"title":"Web Scraping Analysis Function","type":"post"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530169200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530169200,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00-07:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483257600,"objectID":"cd6d9d084287506b4668ad90c6aff50a","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00-08:00","relpermalink":"/talk/example-talk/","section":"talk","summary":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Description This report provides a detailed analysis of the ‘applications’ dataset using supervised fraud algorithms and machine learning statistical techniques. The programming tools used for Data Cleaning, Modeling, and Evaluation were Microsoft Excel, Tableau, R, along with mySQL through R. The original dataset contains unique records of more 90,000 product applications over the year 2016. Necessary feature analysis include: ● Data Understanding and Data Cleaning (dealing with frivolous values) ● Feature Engineering (creating expert Variables, performing feature selection) ● Data Modeling (running fraud detection models) ● Data Evaluation (comparing model results)\nProject Goal Our objective for this project is to use a supervised fraud algorithm to build a fraud detection model that predicts product application fraud with high predictive accuracy. More precisely, we aim to find the top performing machine learning model that is able to predict the highest number of fraudulent cases with a low false positive rate. The report will present several classification machine learning techniques for fraud detection and their performances on our dataset, in complete detail such as Logistic Regression, Random Forests, Support Vector Machine, and Gradient Boosting Trees.\nKey Findings Here are the main key findings from building a Fraud Detection Model on the Applications dataset: ● The Gradient Boosting Trees method yielded the best Fraud Detection model at 10% penetration on the OOT (Out of Time) dataset with a FDR (Fraud Detection Rate) of 13.02% including a FDR of 12.50% on the testing dataset ● The Support Vector Machine Method yielded the lowest Fraud Detection Rate (FDR) at 10% penetration on the OOT (Out of Time) dataset with a FDR (Fraud Detection Rate) of 11.81% including a FDR of 11.01% on the testing dataset ● Overall, Gradient Boosting Trees and Random Forest Models both demonstrated relatively high predictive performance, with the highest predictive power in their respective segments\nClick to Download the full report\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"86243c7cfa5442accb0649241afb8458","permalink":"/project/supervised-learning/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/supervised-learning/","section":"project","summary":"Fraud Analytics: Supervised Learning and Fraud Detection in Product Applications.","tags":["Supervised Machine Learning"],"title":"Fraud Analytics: Fraud Detection Model","type":"project"},{"authors":null,"categories":null,"content":" Description This report provides a detailed analysis of ‘New York City Property Tax Valuation Data’ and also aims at identifying potential fraudulent records from the data set using unsupervised machine learning methods. The programming tools used for Data Cleaning were performed on Microsoft Excel and R. The original dataset contains unique records of more than 1 million properties across the state of New York with 30 different fields, both numerical and categorical. Necessary feature analysis include: ● Data Understanding and Data Cleaning (estimating missing values of each fields) ● Data Modeling (creating expert variables) and Data Evaluation ● Fraud Score Calculation (Heuristic Algorithm and Autoencoder)\nProject Goal Our objective for this project is to use machine learning algorithms such as Autoencoder and Heuristic Algorithms and calculate a fraud score for each of the New York Property data records to analyze anomalies for potential fraud detection. By applying both of the unsupervised machine learning methods, records with high scores show to be potentially fraudulent. The report will explain each step in complete detail.\nKey Findings The two fraud detection algorithms we used, Heuristic and Autoencoder-Based Anomaly Detection, had a considerably high overlap matching percentage among the top 1% of all records: ● Missing values were properly filled using reasonable data cleaning methodology. ● The 51 expert variables were carefully crafted to perform PCA analysis. ● After normalizing the fraud scores and visualizing the scores, we found both score distributions to be right skewed. ● Among the top 10 highest fraud score records, anomalies were found in several fields.\nClick to Download the full report\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"/project/deep-learning/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Fraud Analytics: Unsupervised Learning and Fraud Score Calculation on New York property tax.","tags":["Unsupervised Machine Learning"],"title":"Fraud Analytics: Fraud Score Calculation","type":"project"},{"authors":null,"categories":null,"content":" Executive Summary For several universities across the world, the implementation of a systematic approach to course scheduling remains a problem. Discrete optimization approaches have been used to solve the problem independently at such institutions, however owing to the complex nature of the problem compounded by the viewpoints of various stakeholders, a universal “cookie-cutter” solution does not exist. There are 7 departments in the Marshall School of Business at USC, and course scheduling is currently performed by reusing the course schedule from the previous year as closely as possible and manually implementing a series of reforms to improve efficiency and transparency. The current process assigns over 400 courses into 45 schedulable rooms at 42 different times. However, the schedules produced by the current process are not always transparent and efficient. Additionally, sections are scheduled and de-scheduled multiple times during the course allotment process which makes greedy approaches to the allocation problem inefficient.\nOur solution to the problem involves the creation of a tractable MIP-based allocation tool that equitably assigns departments to classroom-timeslots. An algorithm [1] developed in-house generates classroom-timeslot preference scores for each department based on survey results [2] , and every run of the allocation tool outputs candidate sets of classroom-timeslots for each department. The size of the candidate sets is dependent upon a model-tuning parameter that controls the greediness of the allocation approach. This grants a greater degree of flexibility in course-scheduling and ensures that a certain quantity of classroom-timeslots of each category [3] are available for the next (potential) run of the allocation tool to assign the remaining sections. The focus of this report is to describe the functioning of the tool, and the business value it offers to the administrative wing of the Marshall School of Business.\nIn the first step of the report, we propose a precise and high-level explanation on how the application of Mixed Integer Programming can be used to solve the course scheduling problem. In the second step of the report, we formulate a Mixed Integer Program by creating the input and output data, the decision variables, the objective function and the constraints. We conclude with a description of the output from one run of the model, and why the model makes sense and is easily implementable.\nClick to Download the full report\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"/project/example-external-project/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/example-external-project/","section":"project","summary":"A MIP-based Approach to Optimizing Course Scheduling at USC Marshall","tags":["Optimization"],"title":"Optimizing Course Scheduling","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441090800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441090800,"objectID":"5cec0ce6e082b377c504bc66cdf990c5","permalink":"/publication/person-re-identification/","publishdate":"2015-09-01T00:00:00-07:00","relpermalink":"/publication/person-re-identification/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372662000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372662000,"objectID":"caae70970030052c8f733b2ca8421a2b","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-07:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":["R"],"content":"Loading Data set and libraries library(dplyr) library(ggplot2) library(stringr) library(caret) library(Hmisc) library(randomForest) train \u0026lt;- read.csv(\u0026quot;train.csv\u0026quot;) test \u0026lt;- read.csv(\u0026quot;test.csv\u0026quot;)  Data exploration summary(test) ## PassengerId Pclass ## Min. : 892.0 Min. :1.000 ## 1st Qu.: 996.2 1st Qu.:1.000 ## Median :1100.5 Median :3.000 ## Mean :1100.5 Mean :2.266 ## 3rd Qu.:1204.8 3rd Qu.:3.000 ## Max. :1309.0 Max. :3.000 ## ## Name Sex ## Abbott, Master. Eugene Joseph : 1 female:152 ## Abelseth, Miss. Karen Marie : 1 male :266 ## Abelseth, Mr. Olaus Jorgensen : 1 ## Abrahamsson, Mr. Abraham August Johannes : 1 ## Abrahim, Mrs. Joseph (Sophie Halaut Easu): 1 ## Aks, Master. Philip Frank : 1 ## (Other) :412 ## Age SibSp Parch Ticket ## Min. : 0.17 Min. :0.0000 Min. :0.0000 PC 17608: 5 ## 1st Qu.:21.00 1st Qu.:0.0000 1st Qu.:0.0000 113503 : 4 ## Median :27.00 Median :0.0000 Median :0.0000 CA. 2343: 4 ## Mean :30.27 Mean :0.4474 Mean :0.3923 16966 : 3 ## 3rd Qu.:39.00 3rd Qu.:1.0000 3rd Qu.:0.0000 220845 : 3 ## Max. :76.00 Max. :8.0000 Max. :9.0000 347077 : 3 ## NA\u0026#39;s :86 (Other) :396 ## Fare Cabin Embarked ## Min. : 0.000 :327 C:102 ## 1st Qu.: 7.896 B57 B59 B63 B66: 3 Q: 46 ## Median : 14.454 A34 : 2 S:270 ## Mean : 35.627 B45 : 2 ## 3rd Qu.: 31.500 C101 : 2 ## Max. :512.329 C116 : 2 ## NA\u0026#39;s :1 (Other) : 80 summary(train) ## PassengerId Survived Pclass ## Min. : 1.0 Min. :0.0000 Min. :1.000 ## 1st Qu.:223.5 1st Qu.:0.0000 1st Qu.:2.000 ## Median :446.0 Median :0.0000 Median :3.000 ## Mean :446.0 Mean :0.3838 Mean :2.309 ## 3rd Qu.:668.5 3rd Qu.:1.0000 3rd Qu.:3.000 ## Max. :891.0 Max. :1.0000 Max. :3.000 ## ## Name Sex Age ## Abbing, Mr. Anthony : 1 female:314 Min. : 0.42 ## Abbott, Mr. Rossmore Edward : 1 male :577 1st Qu.:20.12 ## Abbott, Mrs. Stanton (Rosa Hunt) : 1 Median :28.00 ## Abelson, Mr. Samuel : 1 Mean :29.70 ## Abelson, Mrs. Samuel (Hannah Wizosky): 1 3rd Qu.:38.00 ## Adahl, Mr. Mauritz Nils Martin : 1 Max. :80.00 ## (Other) :885 NA\u0026#39;s :177 ## SibSp Parch Ticket Fare ## Min. :0.000 Min. :0.0000 1601 : 7 Min. : 0.00 ## 1st Qu.:0.000 1st Qu.:0.0000 347082 : 7 1st Qu.: 7.91 ## Median :0.000 Median :0.0000 CA. 2343: 7 Median : 14.45 ## Mean :0.523 Mean :0.3816 3101295 : 6 Mean : 32.20 ## 3rd Qu.:1.000 3rd Qu.:0.0000 347088 : 6 3rd Qu.: 31.00 ## Max. :8.000 Max. :6.0000 CA 2144 : 6 Max. :512.33 ## (Other) :852 ## Cabin Embarked ## :687 : 2 ## B96 B98 : 4 C:168 ## C23 C25 C27: 4 Q: 77 ## G6 : 4 S:644 ## C22 C26 : 3 ## D : 3 ## (Other) :186 #We can see that survived is missing on the test data, let\u0026#39;s input it as a NA value for now test \u0026lt;- test %\u0026gt;% mutate(Survived = NA) #Lets merge the two dataset together for data wrangling/cleaning merge_data \u0026lt;- rbind(train, test) #Checking if there are no duplicates length(unique(merge_data$PassengerId)) == 891 + 418 ## [1] TRUE #TRUE Looking at the Survived by Sex\ntrain %\u0026gt;% ggplot(aes(x = Sex, fill = as.factor(Survived)))+ geom_bar()+ geom_text(stat = \u0026#39;count\u0026#39;, aes(label = ..count..))+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;) Looking at the Survived by Pclass\ntrain %\u0026gt;% ggplot(aes(x = as.factor(Pclass), fill = as.factor(Survived)))+ geom_bar()+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Pclass\u0026quot;) LOoking at the Survived by Sibsp\ntrain %\u0026gt;% ggplot(aes(x = as.factor(SibSp), fill = as.factor(Survived)))+ geom_bar()+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;SibSp\u0026quot;) Looking at the Survived by Sibsp and Sex\ntrain %\u0026gt;% ggplot(aes(x = as.factor(SibSp), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;SibSp\u0026quot;) Looking at the Survived by Pclass and Sex\ntrain %\u0026gt;% ggplot(aes(x = as.factor(Pclass), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Pclass\u0026quot;) Looking at the age distribution\nmerge_data %\u0026gt;% ggplot(aes(x = Age))+ geom_histogram(fill = \u0026quot;#000099\u0026quot;, color = \u0026quot;black\u0026quot;) Data understanding and Data cleaning Lets group the age into bucket\n#Lets convert some of the columns name into factors merge_data$Sex \u0026lt;- as.factor(merge_data$Sex) merge_data$Pclass \u0026lt;- as.factor(merge_data$Pclass) #First lets replace the missing values of age by the mean of the subgroup by pclass since it has no missing values merge_data %\u0026gt;% filter(!is.na(Age)) %\u0026gt;% group_by(Pclass) %\u0026gt;% summarise(Age_Pclass_mean = round(mean(Age))) ## # A tibble: 3 x 2 ## Pclass Age_Pclass_mean ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 39 ## 2 2 30 ## 3 3 25 # 1 39 # 2 30 # 3 25 #now lets replace the missing age values merge_data_missing_age \u0026lt;- merge_data %\u0026gt;% filter(is.na(Age)) %\u0026gt;% mutate(Age = ifelse(Pclass == \u0026quot;1\u0026quot;, 39 , ifelse(Pclass == \u0026quot;2\u0026quot;, 30, 25))) %\u0026gt;% select(PassengerId, Age) merge_data \u0026lt;- merge_data %\u0026gt;% left_join(merge_data_missing_age, by = \u0026#39;PassengerId\u0026#39;) %\u0026gt;% mutate(Age = ifelse(is.na(Age.x), Age.y, Age.x)) %\u0026gt;% select(-Age.x, -Age.y) merge_data$Age_bucket = ifelse(merge_data$Age \u0026lt; 12 , \u0026quot;\u0026lt;12\u0026quot;, ifelse(merge_data$Age \u0026gt;= 12 \u0026amp; merge_data$Age \u0026lt; 21, \u0026quot;16-21\u0026quot;, ifelse(merge_data$Age \u0026gt;= 21 \u0026amp; merge_data$Age \u0026lt; 30, \u0026quot;21-30\u0026quot;, ifelse(merge_data$Age \u0026gt;= 30 \u0026amp; merge_data$Age \u0026lt; 40, \u0026quot;30-40\u0026quot;, ifelse(merge_data$Age \u0026gt;= 40 \u0026amp; merge_data$Age \u0026lt; 50, \u0026quot;40-50\u0026quot;,\u0026quot;50+\u0026quot;))))) merge_data %\u0026gt;% group_by(Age_bucket) %\u0026gt;% tally() ## # A tibble: 6 x 2 ## Age_bucket n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 \u0026lt;12 91 ## 2 16-21 158 ## 3 21-30 528 ## 4 30-40 287 ## 5 40-50 135 ## 6 50+ 110 merge_data$Age_bucket \u0026lt;- as.factor(merge_data$Age_bucket) merge_data[1:891,] %\u0026gt;% ggplot(aes(x = as.factor(Age_bucket), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Age Bucket\u0026quot;) Lets extract the first letter from the Cabin\nlength(unique(merge_data$Cabin)) ## [1] 187 merge_data$Cabin_Letter \u0026lt;- str_sub(merge_data$Cabin,1,1) #creating a boolean variable merge_data$Cabin_Available \u0026lt;- as.factor(!(merge_data$Cabin==\u0026quot;\u0026quot;)) Fare\nmerge_data %\u0026gt;% ggplot(aes(x = Fare))+ geom_histogram(fill = \u0026quot;#000099\u0026quot;, color = \u0026quot;black\u0026quot;) sum(is.na(merge_data$Fare)) ## [1] 1 merge_data[which(is.na(merge_data$Fare)),] ## PassengerId Survived Pclass Name Sex SibSp Parch ## 1044 1044 NA 3 Storey, Mr. Thomas male 0 0 ## Ticket Fare Cabin Embarked Age Age_bucket Cabin_Letter ## 1044 3701 NA S 60.5 50+ ## Cabin_Available ## 1044 FALSE #lets replace the fare by mean of Pclass = 3 merge_data[1044,]$Fare \u0026lt;- mean(subset(merge_data,Pclass==3)$Fare,na.rm=TRUE) merge_data$Fare_bucket \u0026lt;- cut2(merge_data$Fare,g=5) merge_data$Fare_bucket \u0026lt;- as.factor(merge_data$Fare_bucket) merge_data[1:891,] %\u0026gt;% ggplot(aes(x = as.factor(Fare_bucket), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Fare Bucket\u0026quot;) Title\nmerge_data$Title \u0026lt;- merge_data$Title \u0026lt;- gsub(\u0026#39;(.*, )|(\\\\..*)\u0026#39;, \u0026#39;\u0026#39;, merge_data$Name) unique(merge_data$Title) ## [1] \u0026quot;Mr\u0026quot; \u0026quot;Mrs\u0026quot; \u0026quot;Miss\u0026quot; \u0026quot;Master\u0026quot; ## [5] \u0026quot;Don\u0026quot; \u0026quot;Rev\u0026quot; \u0026quot;Dr\u0026quot; \u0026quot;Mme\u0026quot; ## [9] \u0026quot;Ms\u0026quot; \u0026quot;Major\u0026quot; \u0026quot;Lady\u0026quot; \u0026quot;Sir\u0026quot; ## [13] \u0026quot;Mlle\u0026quot; \u0026quot;Col\u0026quot; \u0026quot;Capt\u0026quot; \u0026quot;the Countess\u0026quot; ## [17] \u0026quot;Jonkheer\u0026quot; \u0026quot;Dona\u0026quot; #We can see that there some titles that seem to have some luxurious power such as Sir, Major etc... Power_Title \u0026lt;- c(\u0026quot;Capt\u0026quot;,\u0026quot;Col\u0026quot;,\u0026quot;Don\u0026quot;,\u0026quot;Dona\u0026quot;,\u0026quot;Dr\u0026quot;,\u0026quot;Jonkheer\u0026quot;,\u0026quot;Lady\u0026quot;,\u0026quot;Major\u0026quot;, \u0026quot;Mlle\u0026quot;, \u0026quot;Mme\u0026quot;,\u0026quot;Rev\u0026quot;,\u0026quot;Sir\u0026quot;,\u0026quot;the Countess\u0026quot;) merge_data$Title[merge_data$Title %in% Power_Title] \u0026lt;- \u0026quot;Luxurious Title\u0026quot; merge_data$Title \u0026lt;- as.factor(merge_data$Title) Embarked\nmerge_data$Embarked %\u0026gt;% head() ## [1] S C S S S Q ## Levels: C Q S merge_data$Embarked \u0026lt;- as.factor(merge_data$Embarked) str(merge_data) ## \u0026#39;data.frame\u0026#39;: 1309 obs. of 17 variables: ## $ PassengerId : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Survived : int 0 1 1 1 0 0 0 0 1 1 ... ## $ Pclass : Factor w/ 3 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;: 3 1 3 1 3 3 1 3 3 2 ... ## $ Name : Factor w/ 1307 levels \u0026quot;Abbing, Mr. Anthony\u0026quot;,..: 109 191 358 277 16 559 520 629 417 581 ... ## $ Sex : Factor w/ 2 levels \u0026quot;female\u0026quot;,\u0026quot;male\u0026quot;: 2 1 1 1 2 2 2 2 1 1 ... ## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ... ## $ Ticket : Factor w/ 929 levels \u0026quot;110152\u0026quot;,\u0026quot;110413\u0026quot;,..: 524 597 670 50 473 276 86 396 345 133 ... ## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ... ## $ Cabin : Factor w/ 187 levels \u0026quot;\u0026quot;,\u0026quot;A10\u0026quot;,\u0026quot;A14\u0026quot;,..: 1 83 1 57 1 1 131 1 1 1 ... ## $ Embarked : Factor w/ 4 levels \u0026quot;\u0026quot;,\u0026quot;C\u0026quot;,\u0026quot;Q\u0026quot;,\u0026quot;S\u0026quot;: 4 2 4 4 4 3 4 4 4 2 ... ## $ Age : num 22 38 26 35 35 25 54 2 27 14 ... ## $ Age_bucket : Factor w/ 6 levels \u0026quot;\u0026lt;12\u0026quot;,\u0026quot;16-21\u0026quot;,..: 3 4 3 4 4 3 6 1 3 2 ... ## $ Cabin_Letter : chr \u0026quot;\u0026quot; \u0026quot;C\u0026quot; \u0026quot;\u0026quot; \u0026quot;C\u0026quot; ... ## $ Cabin_Available: Factor w/ 2 levels \u0026quot;FALSE\u0026quot;,\u0026quot;TRUE\u0026quot;: 1 2 1 2 1 1 2 1 1 1 ... ## $ Fare_bucket : Factor w/ 5 levels \u0026quot;[ 0.00, 7.88)\u0026quot;,..: 1 5 2 5 2 2 5 3 3 4 ... ## $ Title : Factor w/ 6 levels \u0026quot;Luxurious Title\u0026quot;,..: 4 5 3 5 4 4 4 2 5 5 ... summary(merge_data$Embarked) ## C Q S ## 2 270 123 914 SibSp\nmerge_data$SibSp \u0026lt;- as.factor(merge_data$SibSp) Ticket\nlibrary(plyr) merge_data \u0026lt;- ddply(merge_data,.(Ticket),transform,Ticketsize=length(Ticket)) merge_data$Ticketsize \u0026lt;- as.factor(merge_data$Ticketsize) merge_data \u0026lt;- merge_data %\u0026gt;% arrange(PassengerId)   Data Modeling #resplitting the merge data train_merge \u0026lt;- merge_data[1:891,] test_merge \u0026lt;- merge_data[892:1309,] train_merge$Survived \u0026lt;- as.factor(train_merge$Survived) #splitting the train merge dataset into a build and validate set set.seed(123) smp_size \u0026lt;- floor(0.75* nrow(train_merge)) train_ind \u0026lt;- sample(seq_len(nrow(train_merge)), size = smp_size) train \u0026lt;- train_merge[train_ind, ] test \u0026lt;- train_merge[-train_ind, ] Logistic Regression\nlog.fit \u0026lt;- glm(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + Ticketsize, family = binomial(link = logit), data = train) summary(log.fit) ## ## Call: ## glm(formula = Survived ~ Pclass + Sex + Age_bucket + Fare_bucket + ## Cabin_Available + Title + Embarked + Ticketsize, family = binomial(link = logit), ## data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6555 -0.5228 -0.3730 0.5764 2.3832 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 2.965e+01 2.064e+03 0.014 0.988537 ## Pclass2 -4.734e-01 6.072e-01 -0.780 0.435602 ## Pclass3 -1.471e+00 7.172e-01 -2.051 0.040278 * ## Sexmale -1.701e+01 1.186e+03 -0.014 0.988562 ## Age_bucket16-21 -3.940e-01 6.916e-01 -0.570 0.568934 ## Age_bucket21-30 -5.586e-01 6.550e-01 -0.853 0.393709 ## Age_bucket30-40 -7.430e-01 6.855e-01 -1.084 0.278401 ## Age_bucket40-50 -1.049e+00 7.547e-01 -1.390 0.164491 ## Age_bucket50+ -1.780e+00 7.966e-01 -2.235 0.025415 * ## Fare_bucket[ 7.88, 10.52) 3.411e-01 3.813e-01 0.895 0.371030 ## Fare_bucket[10.52, 22.02) 2.105e-01 4.471e-01 0.471 0.637714 ## Fare_bucket[22.02, 42.40) 4.024e-01 5.740e-01 0.701 0.483282 ## Fare_bucket[42.40,512.33] 9.585e-01 8.862e-01 1.082 0.279444 ## Cabin_AvailableTRUE 7.037e-01 4.225e-01 1.666 0.095805 . ## TitleMaster 3.823e+00 1.113e+00 3.434 0.000595 *** ## TitleMiss -1.325e+01 1.186e+03 -0.011 0.991086 ## TitleMr 9.256e-01 8.392e-01 1.103 0.270024 ## TitleMrs -1.267e+01 1.186e+03 -0.011 0.991480 ## TitleMs 1.907e+00 2.677e+03 0.001 0.999432 ## EmbarkedC -1.363e+01 1.689e+03 -0.008 0.993561 ## EmbarkedQ -1.352e+01 1.689e+03 -0.008 0.993614 ## EmbarkedS -1.417e+01 1.689e+03 -0.008 0.993304 ## Ticketsize2 -5.106e-01 4.145e-01 -1.232 0.217941 ## Ticketsize3 -9.938e-03 5.289e-01 -0.019 0.985009 ## Ticketsize4 2.419e-01 6.942e-01 0.348 0.727549 ## Ticketsize5 -2.147e+00 8.637e-01 -2.486 0.012912 * ## Ticketsize6 -3.588e+00 9.610e-01 -3.733 0.000189 *** ## Ticketsize7 -2.101e+00 9.058e-01 -2.320 0.020340 * ## Ticketsize8 1.712e+00 1.164e+00 1.471 0.141277 ## Ticketsize11 -1.687e+01 7.616e+02 -0.022 0.982323 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 896.48 on 667 degrees of freedom ## Residual deviance: 540.69 on 638 degrees of freedom ## AIC: 600.69 ## ## Number of Fisher Scoring iterations: 15 log_prediction \u0026lt;- predict(log.fit, newdata = test, type = \u0026quot;response\u0026quot;) log_prediction \u0026lt;- ifelse(log_prediction \u0026gt; 0.5 , 1 , 0) logistic_classification_error \u0026lt;- mean(log_prediction != test$Survived) logistic_accuracy \u0026lt;- 1 - logistic_classification_error library(formattable) print(paste0(\u0026quot;The Accuracy for the Logistic Regression is: \u0026quot;, percent(logistic_accuracy))) ## [1] \u0026quot;The Accuracy for the Logistic Regression is: 84.75%\u0026quot;  Let’s use the Caret package to improvde our predictive abilities by using 10-fold cross validation trainControl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3) metric \u0026lt;- \u0026quot;Accuracy\u0026quot; Logistic Regression\nset.seed(123) logistic.fit_2 \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;glm\u0026quot;, metric = metric , trControl = trainControl) KNN\nlibrary(caret) set.seed(123) knn.fit \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;knn\u0026quot;, metric = metric , trControl = trainControl) SVM\nset.seed(123) svm.fit \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;svmRadial\u0026quot;, metric = metric , trControl = trainControl) ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. Naives Bayes\nset.seed(123) naives.fit \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;nb\u0026quot;, metric = metric , trControl = trainControl) results \u0026lt;- resamples(list(Logistic_Regression =logistic.fit_2, KNN=knn.fit, NB=naives.fit, SVM=svm.fit)) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: Logistic_Regression, KNN, NB, SVM ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. ## Logistic_Regression 0.6716418 0.7712906 0.8120760 0.8014830 0.8333333 ## KNN 0.6865672 0.7479947 0.7910448 0.7905373 0.8333333 ## NB 0.5970149 0.6185383 0.6417910 0.6447850 0.6679104 ## SVM 0.6865672 0.7886703 0.8088235 0.8129571 0.8501809 ## Max. NA\u0026#39;s ## Logistic_Regression 0.8656716 0 ## KNN 0.8955224 0 ## NB 0.7462687 0 ## SVM 0.8939394 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. ## Logistic_Regression 0.3092784 0.52664049 0.5967321 0.5826311 0.6481626 ## KNN 0.3446670 0.47209490 0.5527221 0.5510220 0.6368411 ## NB 0.0000000 0.04480394 0.1112105 0.1191201 0.1795026 ## SVM 0.3366337 0.55693939 0.5929342 0.6010410 0.6753950 ## Max. NA\u0026#39;s ## Logistic_Regression 0.7121718 0 ## KNN 0.7752755 0 ## NB 0.4125838 0 ## SVM 0.7793696 0 dotplot(results) From the plot above, we can see that SVm has a the highest accuracy\n SVM Algorithm tuning grid \u0026lt;- expand.grid(.sigma=c(0.005, 0.01, 0.015, 0.02), .C=seq(1, 10, by=1)) set.seed(1234) svm.fit_2 \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket + Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;svmRadial\u0026quot;, metric = metric , trControl = trainControl, tuneGrid = grid) print(svm.fit_2) ## Support Vector Machines with Radial Basis Function Kernel ## ## 668 samples ## 8 predictor ## 2 classes: \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 601, 602, 602, 601, 602, 600, ... ## Resampling results across tuning parameters: ## ## sigma C Accuracy Kappa ## 0.005 1 0.8064433 0.5882477 ## 0.005 2 0.8084187 0.5920974 ## 0.005 3 0.8084187 0.5920974 ## 0.005 4 0.8064433 0.5882477 ## 0.005 5 0.8089386 0.5934264 ## 0.005 6 0.8084335 0.5929460 ## 0.005 7 0.8084335 0.5929460 ## 0.005 8 0.8074385 0.5910867 ## 0.005 9 0.8069410 0.5901764 ## 0.005 10 0.8059384 0.5879319 ## 0.010 1 0.8084187 0.5920974 ## 0.010 2 0.8084335 0.5929460 ## 0.010 3 0.8064435 0.5888920 ## 0.010 4 0.8089836 0.5933946 ## 0.010 5 0.8099414 0.5955983 ## 0.010 6 0.8089612 0.5927757 ## 0.010 7 0.8079737 0.5896887 ## 0.010 8 0.8059761 0.5847156 ## 0.010 9 0.8059613 0.5849894 ## 0.010 10 0.8014458 0.5754778 ## 0.015 1 0.8084187 0.5920974 ## 0.015 2 0.8074908 0.5904415 ## 0.015 3 0.8094514 0.5943612 ## 0.015 4 0.8059686 0.5850598 ## 0.015 5 0.7999603 0.5720062 ## 0.015 6 0.8009482 0.5748111 ## 0.015 7 0.7984605 0.5696365 ## 0.015 8 0.7989657 0.5711440 ## 0.015 9 0.7974878 0.5689145 ## 0.015 10 0.7969826 0.5679644 ## 0.020 1 0.8054631 0.5862188 ## 0.020 2 0.8089466 0.5933911 ## 0.020 3 0.8039632 0.5816605 ## 0.020 4 0.8019657 0.5773838 ## 0.020 5 0.7994779 0.5731117 ## 0.020 6 0.7974951 0.5692218 ## 0.020 7 0.7980150 0.5706871 ## 0.020 8 0.7979851 0.5708311 ## 0.020 9 0.7994706 0.5744198 ## 0.020 10 0.7995005 0.5742534 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.01 and C = 5. plot(svm.fit_2)  Random Forest rf.fit \u0026lt;- randomForest(factor(Survived) ~Pclass + Sex + Age_bucket + Fare_bucket +Cabin_Available + Title + Embarked + Ticketsize, data = train ,nodesize=20) print(rf.fit) ## ## Call: ## randomForest(formula = factor(Survived) ~ Pclass + Sex + Age_bucket + Fare_bucket + Cabin_Available + Title + Embarked + Ticketsize, data = train, nodesize = 20) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 17.96% ## Confusion matrix: ## 0 1 class.error ## 0 362 42 0.1039604 ## 1 78 186 0.2954545 Based on the summary, the random forest model gives us an accuracy of 80.1%\nplot(rf.fit) importance(rf.fit) ## MeanDecreaseGini ## Pclass 14.491398 ## Sex 42.981936 ## Age_bucket 11.592343 ## Fare_bucket 13.527986 ## Cabin_Available 10.630752 ## Title 45.943273 ## Embarked 6.050864 ## Ticketsize 18.833730 rf.fit$confusion ## 0 1 class.error ## 0 362 42 0.1039604 ## 1 78 186 0.2954545 Based on the random forest variable importance, lets only keep the following: Pclass + Sex + Age_bucket+ Fare_bucket Title + Ticketsize\nrf.fit2 \u0026lt;- randomForest(factor(Survived) ~Pclass + Sex + Fare_bucket + Cabin_Available + Ticketsize + Embarked + Title , data = train, nodesize=20) rf.fit2$confusion ## 0 1 class.error ## 0 365 39 0.09653465 ## 1 84 180 0.31818182  Conclusion: Logistic Regression is the winner best_model \u0026lt;- rf.fit prediction \u0026lt;- predict(best_model, test_merge) submission \u0026lt;- data.frame(PassengerId=names(prediction),Survived=prediction) submission %\u0026gt;% head() ## PassengerId Survived ## 892 892 0 ## 893 893 0 ## 894 894 0 ## 895 895 0 ## 896 896 1 ## 897 897 0 write.csv(submission, file = \u0026quot;fourth_submission.csv\u0026quot;, row.names = FALSE)  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"80eb3262d1faa932863a84a67d59db1e","permalink":"/post/titanic_survival/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/titanic_survival/","section":"post","summary":"In this Kaggle Challenge, we apply the tools of machine learning to predict which passengers survived the tragedy.","tags":["R Markdown","Supervised Machine Learning","Data Analysis"],"title":"Titanic Survival","type":"post"},{"authors":null,"categories":["R"],"content":"         From my previous post, I first performed extensive data cleaning in order to analyze the sentiment of each individual cryptocurrency. Here is the result of my analysis.\nlibrary(twitteR) library(dplyr) library(tm) library(wordcloud) library(tidytext) library(tidyverse) library(sqldf) library(ggplot2) library(ggthemes) library(data.table) library(gridExtra) knitr::opts_chunk$set(warning = FALSE, message = FALSE) WordClouds and D3WordClouds Bitcoin\nlibrary(d3wordcloud) big_data_clean_2 \u0026lt;- read.csv(\u0026quot;big_data_clean_2.csv\u0026quot;) bitcoin_words \u0026lt;- big_data_clean_2 %\u0026gt;% filter(coin == \u0026quot;bitcoin\u0026quot;) %\u0026gt;% select(words,words_count) %\u0026gt;% arrange(desc(words_count)) set.seed(123) wordcloud(words = bitcoin_words$words, freq = bitcoin_words$words_count, min.freq = 10, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(10, \u0026quot;Dark2\u0026quot;)) d3wordcloud(bitcoin_words$words, bitcoin_words$words_count, padding = 5, width = \u0026quot;100%\u0026quot;, height = \u0026quot;500px\u0026quot;)  {\"x\":{\"data\":{\"text\":[\"coin\",\"airdrop\",\"mwt\",\"live\",\"wallet\",\"airdropping\",\"ethereum\",\"first\",\"participants\",\"buy\",\"cash\",\"finance\",\"free\",\"litecoin\",\"midas\",\"mining\",\"now\",\"yahoo\",\"0xc73404e3e8961b83a63ef8ec40f41a2b15a9d0fd\",\"market\",\"platform\",\"proc\",\"selfdrop\",\"send\",\"token\",\"trading\",\"will\",\"001\",\"1000\",\"500\",\"africa\",\"back\",\"blockchains\",\"censorship\",\"consequences\",\"corporate\",\"danger\",\"earn\",\"ezscott48\",\"going\",\"integrates\",\"money\",\"new\",\"regulating\",\"see\",\"tch\",\"time\",\"today\",\"traditional\",\"unintended\",\"valued\",\"week\",\"worth$10\",\"$30\",\"$7000\",\"11000\",\"3500\",\"adoption\",\"also\",\"app\",\"ark\",\"asset\",\"beat\",\"big\",\"blockchainbased\",\"bulls\",\"can\",\"cointelegraph\",\"cornucopia\",\"cpollo\",\"device\",\"entrepreneur\",\"everywhere\",\"evolution\",\"excited\",\"faucet\",\"featured\",\"fintech\",\"followback\",\"forex\",\"four\",\"future\",\"good\",\"great\",\"guys\",\"holds\",\"huge\",\"ico\",\"interested\",\"investment\",\"japan\",\"join\",\"jump\",\"know\",\"legal\",\"less\",\"looking\",\"mine\",\"mobile\",\"module\",\"moduleproject\",\"names\",\"next\",\"offering\",\"one\",\"patent\",\"pay\",\"payment\",\"people\",\"pips\",\"presale\",\"procurrency\",\"project\",\"really\",\"reddit\",\"scalabilit\",\"sign\",\"square\",\"storage\",\"tenfoldprotocol\",\"thailand\",\"trade\",\"using\",\"via\",\"want\",\"war\",\"ways\",\"youtube\",\"blockchain\",\"news\",\"get\",\"$100\",\"$100k\",\"$20k\",\"$30to\",\"$351000\",\"$400\",\"$ake\",\"$bigg\",\"$bloc\",\"$blok\",\"100\",\"101\",\"11000proc\",\"130284\",\"168\",\"1retweet\",\"2017\",\"2018\",\"2follow\",\"3000\",\"3500proc\",\"35c3\",\"5000\",\"accept\",\"accepted\",\"accepts\",\"across\",\"add\",\"address\",\"age\",\"aircoins\",\"airdrops\",\"alternative\",\"amazing\",\"analysis\",\"another\",\"application\",\"aqua\",\"aqualite\",\"architect\",\"aset\",\"asian\",\"asked\",\"askingforafriend\",\"assets\",\"augmented\",\"average\",\"awesome\",\"bank\",\"basic\",\"basics\",\"basis\",\"bday\",\"begins\",\"bestdressed\",\"beta\",\"betta\",\"better$$$\",\"binary\",\"binaryoptions\",\"bitads\",\"bitches\",\"bitcofarm\",\"bitcoingood\",\"biweekly\",\"boost\",\"boring\",\"bot\",\"bottomfishing\",\"bou\",\"bounty\",\"bril\",\"btcfxea\",\"build\",\"bullish\",\"bullrun\",\"bullyesq\",\"busted\",\"calvinayre\",\"card\",\"cards\",\"central\",\"ceo\",\"cha\",\"channel\",\"chaos\",\"charges\",\"chartpick\",\"chief\",\"claim\",\"clean\",\"click\",\"close\",\"closed\",\"closing\",\"cloud\",\"cmegroup\",\"cnbc\",\"coal\",\"coinme\",\"combines\",\"come\",\"comes\",\"communication\",\"congress\",\"contest\",\"continues\",\"corginomics\",\"cornucopiaico\",\"countries\",\"countrys\",\"cracked\",\"crowdfunding\",\"cryptoassets\",\"cryptocean\",\"cryptocryptocurrency\",\"cryptocurreency\",\"cryptoisthefuture\",\"cryptokiitten\",\"cryptolife\",\"cryptonews\",\"cryptotonight\",\"cup\",\"currency\",\"current\",\"daily\",\"dangerous\",\"dealer\",\"dealing\",\"decentralized\",\"decide\",\"deck\",\"defenseintel\",\"develop\",\"dictated\",\"digest\",\"distributes\",\"dlt\",\"dodiis17\",\"dodiisww\",\"dogshit\",\"domain\",\"dont\",\"draintheswamp\",\"drew\",\"drive\",\"drop\",\"eagerly\",\"economics\",\"economy\",\"ecosystem\",\"edgewallet\",\"educated\",\"effect\",\"employed\",\"enabling\",\"endless\",\"energy\",\"enjoy\",\"entirely\",\"era\",\"especially\",\"eur\",\"ever\",\"every\",\"expensive\",\"explained\",\"exworkers\",\"facebook\",\"facing\",\"facts\",\"family\",\"farm\",\"fast\",\"feature\",\"federal\",\"fiat\",\"finally\",\"find\",\"followme\",\"forextrade\",\"forget\",\"forums\",\"franchise\",\"freecoins\",\"friday\",\"friends\",\"fuck\",\"funding\",\"futures\",\"gbpusd\",\"getting\",\"getvthqt\",\"giveaway\",\"gold\",\"golden\",\"gonna\",\"got\",\"grand\",\"grassroots\",\"grave\",\"gregorymckenna\",\"grown\",\"gtgt\",\"guide\",\"hang\",\"hardware\",\"havent\",\"hearts\",\"help\",\"holy\",\"httpstco0iaeolmlth\",\"httpstco0vibckuxyu\",\"httpstco0zbuo6p7nh\",\"httpstco1mbuaj77hn\",\"httpstco1xzfzzeegt\",\"httpstco2wvnzgysw9\",\"httpstco3di5u2zcl5\",\"httpstco3tynqwrvzo\",\"httpstco4cmzkgjf9k\",\"httpstco4unrjjj1sj\",\"httpstco5ao0peefyv\",\"httpstco5lddyfky16\",\"httpstco5zswlr9cew\",\"httpstco64qbuzuygd\",\"httpstco6hlsxb0kz4\",\"httpstco6l4v4w179a\",\"httpstco6puimtrkbd\",\"httpstco7hyrtguh6e\",\"httpstco7vbdrwhzcx\",\"httpstco9agzxapztg\",\"httpstco9rnowwbpkl\",\"httpstco9tqm76edlg\",\"httpstco9v3oqqhd24\",\"httpstcoa78ed7h5tk\",\"httpstcoamptdzc3zv\",\"httpstcocaomjwby7z\",\"httpstcocejneye4pw\",\"httpstcocgqhwus6lo\",\"httpstcocjixmjvpiz\",\"httpstcod5dloqlbhk\",\"httpstcod9nasd68kc\",\"httpstcode7q3z8zrh\",\"httpstcodmkqskxlhe\",\"httpstcodnllcef1hh\",\"httpstcodx2smitbrq\",\"httpstcoemeihfjivs\",\"httpstcoepmuhcz396\",\"httpstcoeqgpk6vvo0\",\"httpstcof2svfj9p6e\",\"httpstcof5kcjmgdjk\",\"httpstcof9y7bewhrf\",\"httpstcofraj05jvdz\",\"httpstcogxvxrqs1yq\",\"httpstcohfngkubwcg\",\"httpstcohv3wzj337t\",\"httpstcoiln8acxzho\",\"httpstcoilsbc8qax3\",\"httpstcoiptsrswhlf\",\"httpstcoipwso37ajd\",\"httpstcoiqmsbz9pdr\",\"httpstcoiqsxcr9kkf\",\"httpstcoirunvrpb02\",\"httpstcoivnsoaq1e7\",\"httpstcoiwkqsc0xsj\",\"httpstcojhafxzqbav\",\"httpstcojn7pmkje4m\",\"httpstcojzjpsbqpgp\",\"httpstcokzgcciajfs\",\"httpstcol8jfqeplbx\",\"httpstcolhi81dz2hu\",\"httpstcolirsgcts5e\",\"httpstcolrgcuqnfgp\",\"httpstcomb8caqo3m5\",\"httpstcomczvdtdqqy\",\"httpstcomhk9mwmhat\",\"httpstcomm2w7bljne\",\"httpstcomodezzff8f\",\"httpstcomqzsc2ugys\",\"httpstcomr0eg4lmod\",\"httpstcomw5pdtrygp\",\"httpstcon04ceg38lz\",\"httpstconlq2ki1tqm\",\"httpstcontqofgfpsu\",\"httpstcoohycd2ngae\",\"httpstcooqafqtj1r2\",\"httpstcootvppjcfw9\",\"httpstcopbld2fqnpt\",\"httpstcopgadtx3bx0\",\"httpstcopktjwhnca5\",\"httpstcoqmr7poezai\",\"httpstcoqw8kaskffj\",\"httpstcoqy6cmhg828\",\"httpstcoqzpiwbvmac\",\"httpstcorxqnq1urna\",\"httpstcorypuber3cs\",\"httpstcos0kpgxfqsl\",\"httpstcososewrnbw6\",\"httpstcosrhyg2wij7\",\"httpstcosvuxnigww5\",\"httpstcosz0im9dwur\",\"httpstcot45jrayuix\",\"httpstcotadhsc6jkb\",\"httpstcotcoauiptdy\",\"httpstcotmob8wjver\",\"httpstcotsmjgovlya\",\"httpstcou5zlg3zz5p\",\"httpstcouifeqs80ba\",\"httpstcoul0fmcx78e\",\"httpstcoumdgwtfxtp\",\"httpstcoupow9kvey8\",\"httpstcouzt5gue9wl\",\"httpstcovdno18c8zc\",\"httpstcovimjoszqnu\",\"httpstcovthobvguit\",\"httpstcowfpkkckwn6\",\"httpstcowh5ijgz9mz\",\"httpstcowlugtwuw7o\",\"httpstcoxfyvq7idtr\",\"httpstcoxs42fhc2b1\",\"httpstcoxyzmnbc8he\",\"httpstcoybxd3dfy7t\",\"httpstcoyjzxkoli1k\",\"httpstcoz12xi6pa2ss\",\"httpstcozkir04iqm9\",\"httpstcozujjgzb0nc\",\"httpstcozxumklka0c\",\"httpstcozyeomwlbfs\",\"httpstcozynfgsm1cl\",\"hurt\",\"ilovebitcoin\",\"incorporate\",\"india\",\"industry\",\"institutional\",\"interest\",\"intuittrading\",\"investing\",\"investors\",\"issues\",\"jesus\",\"keep\",\"king\",\"korea\",\"larceny\",\"latte\",\"launching\",\"learn\",\"ledger\",\"lets\",\"lighning\",\"likely\",\"link\",\"loan\",\"lol\",\"lots\",\"love\",\"ltc\",\"machinelearning\",\"maga\",\"make\",\"making\",\"malta\",\"may\",\"meet\",\"might\",\"million\",\"mimblewimble\",\"mira\",\"misconceptions\",\"missing\",\"mission\",\"mobilego\",\"montana\",\"monthly\",\"moon\",\"mooncashfaucet\",\"movement\",\"mumbai\",\"need\",\"needs\",\"netherlands\",\"network\",\"never\",\"noon\",\"oct31\",\"office\",\"officials\",\"open\",\"opinion\",\"order\",\"overtak\",\"pacific\",\"part\",\"payout\",\"petro\",\"physical\",\"pinch\",\"planning\",\"plant\",\"play\",\"polny\",\"positive\",\"possibilities\",\"pow\",\"powered\",\"practical\",\"pro\",\"projects\",\"proof\",\"provides\",\"ptc\",\"pumpkin\",\"purchase\",\"qampa\",\"qanon\",\"quick\",\"raspberrypi\",\"reach\",\"reactions\",\"read\",\"real\",\"reality\",\"record\",\"red\",\"redwave\",\"referral\",\"registration\",\"researching\",\"reserve\",\"revoultion\",\"ridiculous\",\"riding\",\"ripple\",\"rises\",\"road\",\"rosecrypto\",\"satoshilite\",\"says\",\"scheme\",\"scholarship\",\"season\",\"seasons\",\"secure\",\"security\",\"seen\",\"selling\",\"senator\",\"september\",\"services\",\"settled\",\"silver\",\"since\",\"small\",\"solutions\",\"soon\",\"special\",\"spice\",\"splitting\",\"springleap\",\"start\",\"started\",\"startup\",\"stay\",\"stec\",\"steemit\",\"stocks\",\"stores\",\"strategy\",\"street\",\"students\",\"suburb\",\"suc\",\"suggesting\",\"swarm\",\"syste\",\"taiwan\",\"talk\",\"talking\",\"tangible\",\"tax\",\"telling\",\"testing\",\"tether\",\"thank\",\"theory\",\"thing\",\"think\",\"tippr\",\"todays\",\"total\",\"trader\",\"traffic\",\"trezor\",\"true\",\"truly\",\"trump\",\"trump2020\",\"tweet\",\"twice\",\"underestimate\",\"unhappy\",\"unit\",\"university\",\"unnecessary\",\"untested\",\"unveils\",\"update\",\"uptoken\",\"usdt\",\"use\",\"users\",\"vault\",\"venezuelan\",\"verge\",\"video\",\"views\",\"virtual\",\"visit\",\"vote\",\"wait\",\"wall\",\"warning\",\"wars\",\"watching\",\"wave\",\"whats\",\"whole\",\"win\",\"wins\",\"wolf\",\"womenofcrypto\",\"world\",\"worldwide\",\"worries\",\"worth\",\"wow\",\"xbt\",\"yahoofinance\",\"yall\",\"year\",\"yet\"],\"freq\":[11,10,10,9,9,8,8,8,8,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"size\":[11,10,10,9,9,8,8,8,8,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},\"pars\":{\"font\":\"Open Sans\",\"padding\":5,\"rotmin\":-30,\"rotmax\":30,\"tooltip\":false,\"rangesizefont\":[10,90],\"sizescale\":\"linear\",\"colorscale\":\"linear\",\"spiral\":\"archimedean\",\"colors\":null,\"every_word_has_own_color\":false,\"missing_colors\":true,\"label\":null}},\"evals\":[],\"jsHooks\":[]} Ethereum\nethereum_words \u0026lt;- big_data_clean_2 %\u0026gt;% filter(coin == \u0026quot;ethereum\u0026quot;) %\u0026gt;% select(words,words_count) %\u0026gt;% arrange(desc(words_count)) set.seed(123) wordcloud(words = ethereum_words$words, freq = ethereum_words$words_count, min.freq = 10, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(10, \u0026quot;Dark2\u0026quot;)) Litecoin\nlitecoin_words \u0026lt;- big_data_clean_2 %\u0026gt;% filter(coin == \u0026quot;litecoin\u0026quot;) %\u0026gt;% select(words,words_count) %\u0026gt;% arrange(desc(words_count)) set.seed(123) wordcloud(words = litecoin_words$words, freq = litecoin_words$words_count, min.freq = 10, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(10, \u0026quot;Dark2\u0026quot;))  Sentiment Analysis Bitcoin\ndata_crypto_sentiment \u0026lt;- read.csv(\u0026quot;data_crypto_sentiment.csv\u0026quot;) plotly::ggplotly(data_crypto_sentiment %\u0026gt;% filter(coin == \u0026quot;bitcoin\u0026quot; \u0026amp; !is.na(sentiment)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) %\u0026gt;% ggplot(aes( x = reorder(as.character(sentiment), count), y = count))+ geom_bar(stat = \u0026#39;identity\u0026#39;, fill = \u0026quot;#4271AE\u0026quot;)+ coord_flip()+ labs(title=\u0026#39;Bitcoin Sentiments\u0026#39;, x=\u0026#39;Sentiments\u0026#39;,y=\u0026#39;Sentiment Count\u0026#39;)+ theme_economist())  {\"x\":{\"data\":[{\"orientation\":\"h\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[0,1,3,6,7,8,11,11,13,16,21,25,56,82],\"y\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"text\":[\"reorder(as.character(sentiment), count): superfluous\ncount: 0\",\"reorder(as.character(sentiment), count): constraining\ncount: 1\",\"reorder(as.character(sentiment), count): litigious\ncount: 3\",\"reorder(as.character(sentiment), count): uncertainty\ncount: 6\",\"reorder(as.character(sentiment), count): disgust\ncount: 7\",\"reorder(as.character(sentiment), count): surprise\ncount: 8\",\"reorder(as.character(sentiment), count): anger\ncount: 11\",\"reorder(as.character(sentiment), count): sadness\ncount: 11\",\"reorder(as.character(sentiment), count): fear\ncount: 13\",\"reorder(as.character(sentiment), count): joy\ncount: 16\",\"reorder(as.character(sentiment), count): anticipation\ncount: 21\",\"reorder(as.character(sentiment), count): trust\ncount: 25\",\"reorder(as.character(sentiment), count): negative\ncount: 56\",\"reorder(as.character(sentiment), count): positive\ncount: 82\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(66,113,174,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":51.8655043586551,\"r\":13.2835201328352,\"b\":35.8655043586551,\"l\":99.626400996264},\"plot_bgcolor\":\"transparent\",\"paper_bgcolor\":\"rgba(213,228,235,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"title\":\" Bitcoin Sentiments \",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":19.9252801992528},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4.1,86.1],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"tickvals\":[0,20,40,60,80],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(0,0,0,1)\",\"ticklen\":-6.6417600664176,\"tickwidth\":0.603796369674327,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.483037095739462,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Sentiment Count\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,14.6],\"tickmode\":\"array\",\"ticktext\":[\"superfluous\",\"constraining\",\"litigious\",\"uncertainty\",\"disgust\",\"surprise\",\"anger\",\"sadness\",\"fear\",\"joy\",\"anticipation\",\"trust\",\"negative\",\"positive\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"categoryorder\":\"array\",\"categoryarray\":[\"superfluous\",\"constraining\",\"litigious\",\"uncertainty\",\"disgust\",\"surprise\",\"anger\",\"sadness\",\"fear\",\"joy\",\"anticipation\",\"trust\",\"negative\",\"positive\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":-6.6417600664176,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":1.05664364693007,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Sentiments\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"transparent\",\"bordercolor\":\"transparent\",\"borderwidth\":1.71796707229778,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":16.604400166044}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n // is this being viewed in RStudio?\\n if (location.search == '?viewer_pane=1') {\\n alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n } else {\\n window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n }\\n }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"1b013f187089\":{\"x\":{},\"y\":{},\"type\":\"bar\"}},\"cur_data\":\"1b013f187089\",\"visdat\":{\"1b013f187089\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]} Ethereum\nplotly::ggplotly(data_crypto_sentiment %\u0026gt;% filter(coin == \u0026quot;ethereum\u0026quot; \u0026amp; !is.na(sentiment)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) %\u0026gt;% ggplot(aes( x = reorder(as.character(sentiment), count), y = count))+ geom_bar(stat = \u0026#39;identity\u0026#39;, fill = \u0026quot;#4271AE\u0026quot;)+ coord_flip()+ labs(title=\u0026#39;Ethereum Sentiments\u0026#39;, x=\u0026#39;Sentiments\u0026#39;,y=\u0026#39;Sentiment Count\u0026#39;)+ theme_economist())  {\"x\":{\"data\":[{\"orientation\":\"h\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[0,1,3,4,4,6,6,6,8,15,25,26,27,92],\"y\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"text\":[\"reorder(as.character(sentiment), count): superfluous\ncount: 0\",\"reorder(as.character(sentiment), count): constraining\ncount: 1\",\"reorder(as.character(sentiment), count): litigious\ncount: 3\",\"reorder(as.character(sentiment), count): disgust\ncount: 4\",\"reorder(as.character(sentiment), count): uncertainty\ncount: 4\",\"reorder(as.character(sentiment), count): anger\ncount: 6\",\"reorder(as.character(sentiment), count): sadness\ncount: 6\",\"reorder(as.character(sentiment), count): surprise\ncount: 6\",\"reorder(as.character(sentiment), count): fear\ncount: 8\",\"reorder(as.character(sentiment), count): joy\ncount: 15\",\"reorder(as.character(sentiment), count): negative\ncount: 25\",\"reorder(as.character(sentiment), count): trust\ncount: 26\",\"reorder(as.character(sentiment), count): anticipation\ncount: 27\",\"reorder(as.character(sentiment), count): positive\ncount: 92\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(66,113,174,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":51.8655043586551,\"r\":13.2835201328352,\"b\":35.8655043586551,\"l\":99.626400996264},\"plot_bgcolor\":\"transparent\",\"paper_bgcolor\":\"rgba(213,228,235,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"title\":\" Ethereum Sentiments \",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":19.9252801992528},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4.6,96.6],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"25\",\"50\",\"75\"],\"tickvals\":[0,25,50,75],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"25\",\"50\",\"75\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(0,0,0,1)\",\"ticklen\":-6.6417600664176,\"tickwidth\":0.603796369674327,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.483037095739462,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Sentiment Count\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,14.6],\"tickmode\":\"array\",\"ticktext\":[\"superfluous\",\"constraining\",\"litigious\",\"disgust\",\"uncertainty\",\"anger\",\"sadness\",\"surprise\",\"fear\",\"joy\",\"negative\",\"trust\",\"anticipation\",\"positive\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"categoryorder\":\"array\",\"categoryarray\":[\"superfluous\",\"constraining\",\"litigious\",\"disgust\",\"uncertainty\",\"anger\",\"sadness\",\"surprise\",\"fear\",\"joy\",\"negative\",\"trust\",\"anticipation\",\"positive\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":-6.6417600664176,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":1.05664364693007,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Sentiments\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"transparent\",\"bordercolor\":\"transparent\",\"borderwidth\":1.71796707229778,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":16.604400166044}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n // is this being viewed in RStudio?\\n if (location.search == '?viewer_pane=1') {\\n alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n } else {\\n window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n }\\n }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"1b0136afd43c\":{\"x\":{},\"y\":{},\"type\":\"bar\"}},\"cur_data\":\"1b0136afd43c\",\"visdat\":{\"1b0136afd43c\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]} Litecoin\nplotly::ggplotly(data_crypto_sentiment %\u0026gt;% filter(coin == \u0026quot;litecoin\u0026quot; \u0026amp; !is.na(sentiment)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) %\u0026gt;% ggplot(aes( x = reorder(as.character(sentiment), count), y = count))+ geom_bar(stat = \u0026#39;identity\u0026#39;, fill = \u0026quot;#4271AE\u0026quot;)+ coord_flip()+ labs(title=\u0026#39;Litecoin Sentiments\u0026#39;, x=\u0026#39;Sentiments\u0026#39;,y=\u0026#39;Sentiment Count\u0026#39;)+ theme_economist())  {\"x\":{\"data\":[{\"orientation\":\"h\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[0,0,0,2,5,5,8,11,12,12,18,24,25,80],\"y\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"text\":[\"reorder(as.character(sentiment), count): constraining\ncount: 0\",\"reorder(as.character(sentiment), count): litigious\ncount: 0\",\"reorder(as.character(sentiment), count): superfluous\ncount: 0\",\"reorder(as.character(sentiment), count): uncertainty\ncount: 2\",\"reorder(as.character(sentiment), count): disgust\ncount: 5\",\"reorder(as.character(sentiment), count): sadness\ncount: 5\",\"reorder(as.character(sentiment), count): surprise\ncount: 8\",\"reorder(as.character(sentiment), count): anger\ncount: 11\",\"reorder(as.character(sentiment), count): fear\ncount: 12\",\"reorder(as.character(sentiment), count): joy\ncount: 12\",\"reorder(as.character(sentiment), count): trust\ncount: 18\",\"reorder(as.character(sentiment), count): anticipation\ncount: 24\",\"reorder(as.character(sentiment), count): negative\ncount: 25\",\"reorder(as.character(sentiment), count): positive\ncount: 80\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(66,113,174,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":51.8655043586551,\"r\":13.2835201328352,\"b\":35.8655043586551,\"l\":99.626400996264},\"plot_bgcolor\":\"transparent\",\"paper_bgcolor\":\"rgba(213,228,235,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"title\":\" Litecoin Sentiments \",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":19.9252801992528},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4,84],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"tickvals\":[0,20,40,60,80],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(0,0,0,1)\",\"ticklen\":-6.6417600664176,\"tickwidth\":0.603796369674327,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.483037095739462,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Sentiment Count\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,14.6],\"tickmode\":\"array\",\"ticktext\":[\"constraining\",\"litigious\",\"superfluous\",\"uncertainty\",\"disgust\",\"sadness\",\"surprise\",\"anger\",\"fear\",\"joy\",\"trust\",\"anticipation\",\"negative\",\"positive\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"categoryorder\":\"array\",\"categoryarray\":[\"constraining\",\"litigious\",\"superfluous\",\"uncertainty\",\"disgust\",\"sadness\",\"surprise\",\"anger\",\"fear\",\"joy\",\"trust\",\"anticipation\",\"negative\",\"positive\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":-6.6417600664176,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":1.05664364693007,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Sentiments\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"transparent\",\"bordercolor\":\"transparent\",\"borderwidth\":1.71796707229778,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":16.604400166044}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n // is this being viewed in RStudio?\\n if (location.search == '?viewer_pane=1') {\\n alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n } else {\\n window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n }\\n }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"1b013bd2f878\":{\"x\":{},\"y\":{},\"type\":\"bar\"}},\"cur_data\":\"1b013bd2f878\",\"visdat\":{\"1b013bd2f878\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2bc8a9bee9d2601005946bbad58ed3d9","permalink":"/post/twitter_sentiment_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/twitter_sentiment_analysis/","section":"post","summary":"After performingn extensive data cleaning, I can now use the created web-scraping function that I built on my previous post to show the result of my sentiment analysis","tags":["R Markdown","Web-scraping Analysis","Sentiment Analysis"],"title":"Twitter Sentiment Analysis","type":"post"}]