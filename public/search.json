[{"authors":null,"categories":["R"],"content":"         From my previous post, I first performed extensive data cleaning in order to analyze the sentiment of each individual cryptocurrency. Here is the result of my analysis.\nlibrary(twitteR) library(dplyr) library(tm) library(wordcloud) library(tidytext) library(tidyverse) library(sqldf) library(ggplot2) library(ggthemes) library(data.table) library(gridExtra) library(data.table) library(readr) knitr::opts_chunk$set(warning = FALSE, message = FALSE) WordClouds and D3WordClouds Bitcoin\nlibrary(d3wordcloud) big_data_clean_2 \u0026lt;- read_csv(\u0026quot;../../static/data/big_data_clean_2.csv\u0026quot;) bitcoin_words \u0026lt;- big_data_clean_2 %\u0026gt;% filter(coin == \u0026quot;bitcoin\u0026quot;) %\u0026gt;% select(words,words_count) %\u0026gt;% arrange(desc(words_count)) set.seed(123) wordcloud(words = bitcoin_words$words, freq = bitcoin_words$words_count, min.freq = 10, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(10, \u0026quot;Dark2\u0026quot;)) d3wordcloud(bitcoin_words$words, bitcoin_words$words_count, padding = 5, width = \u0026quot;100%\u0026quot;, height = \u0026quot;500px\u0026quot;)  {\"x\":{\"data\":{\"text\":[\"coin\",\"airdrop\",\"mwt\",\"live\",\"wallet\",\"airdropping\",\"ethereum\",\"first\",\"participants\",\"buy\",\"cash\",\"finance\",\"free\",\"litecoin\",\"midas\",\"mining\",\"now\",\"yahoo\",\"0xc73404e3e8961b83a63ef8ec40f41a2b15a9d0fd\",\"market\",\"platform\",\"proc\",\"selfdrop\",\"send\",\"token\",\"trading\",\"will\",\"001\",\"1000\",\"500\",\"africa\",\"back\",\"blockchains\",\"censorship\",\"consequences\",\"corporate\",\"danger\",\"earn\",\"ezscott48\",\"going\",\"integrates\",\"money\",\"new\",\"regulating\",\"see\",\"tch\",\"time\",\"today\",\"traditional\",\"unintended\",\"valued\",\"week\",\"worth$10\",\"$30\",\"$7000\",\"11000\",\"3500\",\"adoption\",\"also\",\"app\",\"ark\",\"asset\",\"beat\",\"big\",\"blockchainbased\",\"bulls\",\"can\",\"cointelegraph\",\"cornucopia\",\"cpollo\",\"device\",\"entrepreneur\",\"everywhere\",\"evolution\",\"excited\",\"faucet\",\"featured\",\"fintech\",\"followback\",\"forex\",\"four\",\"future\",\"good\",\"great\",\"guys\",\"holds\",\"huge\",\"ico\",\"interested\",\"investment\",\"japan\",\"join\",\"jump\",\"know\",\"legal\",\"less\",\"looking\",\"mine\",\"mobile\",\"module\",\"moduleproject\",\"names\",\"next\",\"offering\",\"one\",\"patent\",\"pay\",\"payment\",\"people\",\"pips\",\"presale\",\"procurrency\",\"project\",\"really\",\"reddit\",\"scalabilit\",\"sign\",\"square\",\"storage\",\"tenfoldprotocol\",\"thailand\",\"trade\",\"using\",\"via\",\"want\",\"war\",\"ways\",\"youtube\",\"blockchain\",\"news\",\"get\",\"$100\",\"$100k\",\"$20k\",\"$30to\",\"$351000\",\"$400\",\"$ake\",\"$bigg\",\"$bloc\",\"$blok\",\"100\",\"101\",\"11000proc\",\"130284\",\"168\",\"1retweet\",\"2017\",\"2018\",\"2follow\",\"3000\",\"3500proc\",\"35c3\",\"5000\",\"accept\",\"accepted\",\"accepts\",\"across\",\"add\",\"address\",\"age\",\"aircoins\",\"airdrops\",\"alternative\",\"amazing\",\"analysis\",\"another\",\"application\",\"aqua\",\"aqualite\",\"architect\",\"aset\",\"asian\",\"asked\",\"askingforafriend\",\"assets\",\"augmented\",\"average\",\"awesome\",\"bank\",\"basic\",\"basics\",\"basis\",\"bday\",\"begins\",\"bestdressed\",\"beta\",\"betta\",\"better$$$\",\"binary\",\"binaryoptions\",\"bitads\",\"bitches\",\"bitcofarm\",\"bitcoingood\",\"biweekly\",\"boost\",\"boring\",\"bot\",\"bottomfishing\",\"bou\",\"bounty\",\"bril\",\"btcfxea\",\"build\",\"bullish\",\"bullrun\",\"bullyesq\",\"busted\",\"calvinayre\",\"card\",\"cards\",\"central\",\"ceo\",\"cha\",\"channel\",\"chaos\",\"charges\",\"chartpick\",\"chief\",\"claim\",\"clean\",\"click\",\"close\",\"closed\",\"closing\",\"cloud\",\"cmegroup\",\"cnbc\",\"coal\",\"coinme\",\"combines\",\"come\",\"comes\",\"communication\",\"congress\",\"contest\",\"continues\",\"corginomics\",\"cornucopiaico\",\"countries\",\"countrys\",\"cracked\",\"crowdfunding\",\"cryptoassets\",\"cryptocean\",\"cryptocryptocurrency\",\"cryptocurreency\",\"cryptoisthefuture\",\"cryptokiitten\",\"cryptolife\",\"cryptonews\",\"cryptotonight\",\"cup\",\"currency\",\"current\",\"daily\",\"dangerous\",\"dealer\",\"dealing\",\"decentralized\",\"decide\",\"deck\",\"defenseintel\",\"develop\",\"dictated\",\"digest\",\"distributes\",\"dlt\",\"dodiis17\",\"dodiisww\",\"dogshit\",\"domain\",\"dont\",\"draintheswamp\",\"drew\",\"drive\",\"drop\",\"eagerly\",\"economics\",\"economy\",\"ecosystem\",\"edgewallet\",\"educated\",\"effect\",\"employed\",\"enabling\",\"endless\",\"energy\",\"enjoy\",\"entirely\",\"era\",\"especially\",\"eur\",\"ever\",\"every\",\"expensive\",\"explained\",\"exworkers\",\"facebook\",\"facing\",\"facts\",\"family\",\"farm\",\"fast\",\"feature\",\"federal\",\"fiat\",\"finally\",\"find\",\"followme\",\"forextrade\",\"forget\",\"forums\",\"franchise\",\"freecoins\",\"friday\",\"friends\",\"fuck\",\"funding\",\"futures\",\"gbpusd\",\"getting\",\"getvthqt\",\"giveaway\",\"gold\",\"golden\",\"gonna\",\"got\",\"grand\",\"grassroots\",\"grave\",\"gregorymckenna\",\"grown\",\"gtgt\",\"guide\",\"hang\",\"hardware\",\"havent\",\"hearts\",\"help\",\"holy\",\"httpstco0iaeolmlth\",\"httpstco0vibckuxyu\",\"httpstco0zbuo6p7nh\",\"httpstco1mbuaj77hn\",\"httpstco1xzfzzeegt\",\"httpstco2wvnzgysw9\",\"httpstco3di5u2zcl5\",\"httpstco3tynqwrvzo\",\"httpstco4cmzkgjf9k\",\"httpstco4unrjjj1sj\",\"httpstco5ao0peefyv\",\"httpstco5lddyfky16\",\"httpstco5zswlr9cew\",\"httpstco64qbuzuygd\",\"httpstco6hlsxb0kz4\",\"httpstco6l4v4w179a\",\"httpstco6puimtrkbd\",\"httpstco7hyrtguh6e\",\"httpstco7vbdrwhzcx\",\"httpstco9agzxapztg\",\"httpstco9rnowwbpkl\",\"httpstco9tqm76edlg\",\"httpstco9v3oqqhd24\",\"httpstcoa78ed7h5tk\",\"httpstcoamptdzc3zv\",\"httpstcocaomjwby7z\",\"httpstcocejneye4pw\",\"httpstcocgqhwus6lo\",\"httpstcocjixmjvpiz\",\"httpstcod5dloqlbhk\",\"httpstcod9nasd68kc\",\"httpstcode7q3z8zrh\",\"httpstcodmkqskxlhe\",\"httpstcodnllcef1hh\",\"httpstcodx2smitbrq\",\"httpstcoemeihfjivs\",\"httpstcoepmuhcz396\",\"httpstcoeqgpk6vvo0\",\"httpstcof2svfj9p6e\",\"httpstcof5kcjmgdjk\",\"httpstcof9y7bewhrf\",\"httpstcofraj05jvdz\",\"httpstcogxvxrqs1yq\",\"httpstcohfngkubwcg\",\"httpstcohv3wzj337t\",\"httpstcoiln8acxzho\",\"httpstcoilsbc8qax3\",\"httpstcoiptsrswhlf\",\"httpstcoipwso37ajd\",\"httpstcoiqmsbz9pdr\",\"httpstcoiqsxcr9kkf\",\"httpstcoirunvrpb02\",\"httpstcoivnsoaq1e7\",\"httpstcoiwkqsc0xsj\",\"httpstcojhafxzqbav\",\"httpstcojn7pmkje4m\",\"httpstcojzjpsbqpgp\",\"httpstcokzgcciajfs\",\"httpstcol8jfqeplbx\",\"httpstcolhi81dz2hu\",\"httpstcolirsgcts5e\",\"httpstcolrgcuqnfgp\",\"httpstcomb8caqo3m5\",\"httpstcomczvdtdqqy\",\"httpstcomhk9mwmhat\",\"httpstcomm2w7bljne\",\"httpstcomodezzff8f\",\"httpstcomqzsc2ugys\",\"httpstcomr0eg4lmod\",\"httpstcomw5pdtrygp\",\"httpstcon04ceg38lz\",\"httpstconlq2ki1tqm\",\"httpstcontqofgfpsu\",\"httpstcoohycd2ngae\",\"httpstcooqafqtj1r2\",\"httpstcootvppjcfw9\",\"httpstcopbld2fqnpt\",\"httpstcopgadtx3bx0\",\"httpstcopktjwhnca5\",\"httpstcoqmr7poezai\",\"httpstcoqw8kaskffj\",\"httpstcoqy6cmhg828\",\"httpstcoqzpiwbvmac\",\"httpstcorxqnq1urna\",\"httpstcorypuber3cs\",\"httpstcos0kpgxfqsl\",\"httpstcososewrnbw6\",\"httpstcosrhyg2wij7\",\"httpstcosvuxnigww5\",\"httpstcosz0im9dwur\",\"httpstcot45jrayuix\",\"httpstcotadhsc6jkb\",\"httpstcotcoauiptdy\",\"httpstcotmob8wjver\",\"httpstcotsmjgovlya\",\"httpstcou5zlg3zz5p\",\"httpstcouifeqs80ba\",\"httpstcoul0fmcx78e\",\"httpstcoumdgwtfxtp\",\"httpstcoupow9kvey8\",\"httpstcouzt5gue9wl\",\"httpstcovdno18c8zc\",\"httpstcovimjoszqnu\",\"httpstcovthobvguit\",\"httpstcowfpkkckwn6\",\"httpstcowh5ijgz9mz\",\"httpstcowlugtwuw7o\",\"httpstcoxfyvq7idtr\",\"httpstcoxs42fhc2b1\",\"httpstcoxyzmnbc8he\",\"httpstcoybxd3dfy7t\",\"httpstcoyjzxkoli1k\",\"httpstcoz12xi6pa2ss\",\"httpstcozkir04iqm9\",\"httpstcozujjgzb0nc\",\"httpstcozxumklka0c\",\"httpstcozyeomwlbfs\",\"httpstcozynfgsm1cl\",\"hurt\",\"ilovebitcoin\",\"incorporate\",\"india\",\"industry\",\"institutional\",\"interest\",\"intuittrading\",\"investing\",\"investors\",\"issues\",\"jesus\",\"keep\",\"king\",\"korea\",\"larceny\",\"latte\",\"launching\",\"learn\",\"ledger\",\"lets\",\"lighning\",\"likely\",\"link\",\"loan\",\"lol\",\"lots\",\"love\",\"ltc\",\"machinelearning\",\"maga\",\"make\",\"making\",\"malta\",\"may\",\"meet\",\"might\",\"million\",\"mimblewimble\",\"mira\",\"misconceptions\",\"missing\",\"mission\",\"mobilego\",\"montana\",\"monthly\",\"moon\",\"mooncashfaucet\",\"movement\",\"mumbai\",\"need\",\"needs\",\"netherlands\",\"network\",\"never\",\"noon\",\"oct31\",\"office\",\"officials\",\"open\",\"opinion\",\"order\",\"overtak\",\"pacific\",\"part\",\"payout\",\"petro\",\"physical\",\"pinch\",\"planning\",\"plant\",\"play\",\"polny\",\"positive\",\"possibilities\",\"pow\",\"powered\",\"practical\",\"pro\",\"projects\",\"proof\",\"provides\",\"ptc\",\"pumpkin\",\"purchase\",\"qampa\",\"qanon\",\"quick\",\"raspberrypi\",\"reach\",\"reactions\",\"read\",\"real\",\"reality\",\"record\",\"red\",\"redwave\",\"referral\",\"registration\",\"researching\",\"reserve\",\"revoultion\",\"ridiculous\",\"riding\",\"ripple\",\"rises\",\"road\",\"rosecrypto\",\"satoshilite\",\"says\",\"scheme\",\"scholarship\",\"season\",\"seasons\",\"secure\",\"security\",\"seen\",\"selling\",\"senator\",\"september\",\"services\",\"settled\",\"silver\",\"since\",\"small\",\"solutions\",\"soon\",\"special\",\"spice\",\"splitting\",\"springleap\",\"start\",\"started\",\"startup\",\"stay\",\"stec\",\"steemit\",\"stocks\",\"stores\",\"strategy\",\"street\",\"students\",\"suburb\",\"suc\",\"suggesting\",\"swarm\",\"syste\",\"taiwan\",\"talk\",\"talking\",\"tangible\",\"tax\",\"telling\",\"testing\",\"tether\",\"thank\",\"theory\",\"thing\",\"think\",\"tippr\",\"todays\",\"total\",\"trader\",\"traffic\",\"trezor\",\"true\",\"truly\",\"trump\",\"trump2020\",\"tweet\",\"twice\",\"underestimate\",\"unhappy\",\"unit\",\"university\",\"unnecessary\",\"untested\",\"unveils\",\"update\",\"uptoken\",\"usdt\",\"use\",\"users\",\"vault\",\"venezuelan\",\"verge\",\"video\",\"views\",\"virtual\",\"visit\",\"vote\",\"wait\",\"wall\",\"warning\",\"wars\",\"watching\",\"wave\",\"whats\",\"whole\",\"win\",\"wins\",\"wolf\",\"womenofcrypto\",\"world\",\"worldwide\",\"worries\",\"worth\",\"wow\",\"xbt\",\"yahoofinance\",\"yall\",\"year\",\"yet\"],\"freq\":[11,10,10,9,9,8,8,8,8,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"size\":[11,10,10,9,9,8,8,8,8,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},\"pars\":{\"font\":\"Open Sans\",\"padding\":5,\"rotmin\":-30,\"rotmax\":30,\"tooltip\":false,\"rangesizefont\":[10,90],\"sizescale\":\"linear\",\"colorscale\":\"linear\",\"spiral\":\"archimedean\",\"colors\":null,\"every_word_has_own_color\":false,\"missing_colors\":true,\"label\":null}},\"evals\":[],\"jsHooks\":[]} Ethereum\nethereum_words \u0026lt;- big_data_clean_2 %\u0026gt;% filter(coin == \u0026quot;ethereum\u0026quot;) %\u0026gt;% select(words,words_count) %\u0026gt;% arrange(desc(words_count)) set.seed(123) wordcloud(words = ethereum_words$words, freq = ethereum_words$words_count, min.freq = 10, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(10, \u0026quot;Dark2\u0026quot;)) Litecoin\nlitecoin_words \u0026lt;- big_data_clean_2 %\u0026gt;% filter(coin == \u0026quot;litecoin\u0026quot;) %\u0026gt;% select(words,words_count) %\u0026gt;% arrange(desc(words_count)) set.seed(123) wordcloud(words = litecoin_words$words, freq = litecoin_words$words_count, min.freq = 10, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(10, \u0026quot;Dark2\u0026quot;))  Sentiment Analysis Bitcoin\ndata_crypto_sentiment \u0026lt;- read_csv(\u0026quot;../../static/data/data_crypto_sentiment.csv\u0026quot;) plotly::ggplotly(data_crypto_sentiment %\u0026gt;% filter(coin == \u0026quot;bitcoin\u0026quot; \u0026amp; !is.na(sentiment)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) %\u0026gt;% ggplot(aes( x = reorder(as.character(sentiment), count), y = count))+ geom_bar(stat = \u0026#39;identity\u0026#39;, fill = \u0026quot;#4271AE\u0026quot;)+ coord_flip()+ labs(title=\u0026#39;Bitcoin Sentiments\u0026#39;, x=\u0026#39;Sentiments\u0026#39;,y=\u0026#39;Sentiment Count\u0026#39;)+ theme_economist())  {\"x\":{\"data\":[{\"orientation\":\"h\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[1,3,6,7,8,11,11,13,16,21,25,56,82],\"y\":[1,2,3,4,5,6,7,8,9,10,11,12,13],\"text\":[\"reorder(as.character(sentiment), count): constraining\ncount: 1\",\"reorder(as.character(sentiment), count): litigious\ncount: 3\",\"reorder(as.character(sentiment), count): uncertainty\ncount: 6\",\"reorder(as.character(sentiment), count): disgust\ncount: 7\",\"reorder(as.character(sentiment), count): surprise\ncount: 8\",\"reorder(as.character(sentiment), count): anger\ncount: 11\",\"reorder(as.character(sentiment), count): sadness\ncount: 11\",\"reorder(as.character(sentiment), count): fear\ncount: 13\",\"reorder(as.character(sentiment), count): joy\ncount: 16\",\"reorder(as.character(sentiment), count): anticipation\ncount: 21\",\"reorder(as.character(sentiment), count): trust\ncount: 25\",\"reorder(as.character(sentiment), count): negative\ncount: 56\",\"reorder(as.character(sentiment), count): positive\ncount: 82\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(66,113,174,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":51.8655043586551,\"r\":13.2835201328352,\"b\":35.8655043586551,\"l\":99.626400996264},\"plot_bgcolor\":\"transparent\",\"paper_bgcolor\":\"rgba(213,228,235,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"title\":\" Bitcoin Sentiments \",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":19.9252801992528},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4.1,86.1],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"tickvals\":[0,20,40,60,80],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(0,0,0,1)\",\"ticklen\":-6.6417600664176,\"tickwidth\":0.603796369674327,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.483037095739462,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Sentiment Count\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,13.6],\"tickmode\":\"array\",\"ticktext\":[\"constraining\",\"litigious\",\"uncertainty\",\"disgust\",\"surprise\",\"anger\",\"sadness\",\"fear\",\"joy\",\"anticipation\",\"trust\",\"negative\",\"positive\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13],\"categoryorder\":\"array\",\"categoryarray\":[\"constraining\",\"litigious\",\"uncertainty\",\"disgust\",\"surprise\",\"anger\",\"sadness\",\"fear\",\"joy\",\"anticipation\",\"trust\",\"negative\",\"positive\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":-6.6417600664176,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":1.05664364693007,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Sentiments\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"transparent\",\"bordercolor\":\"transparent\",\"borderwidth\":1.71796707229778,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":16.604400166044}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n // is this being viewed in RStudio?\\n if (location.search == '?viewer_pane=1') {\\n alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n } else {\\n window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n }\\n }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"176374295230\":{\"x\":{},\"y\":{},\"type\":\"bar\"}},\"cur_data\":\"176374295230\",\"visdat\":{\"176374295230\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]} Ethereum\nplotly::ggplotly(data_crypto_sentiment %\u0026gt;% filter(coin == \u0026quot;ethereum\u0026quot; \u0026amp; !is.na(sentiment)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) %\u0026gt;% ggplot(aes( x = reorder(as.character(sentiment), count), y = count))+ geom_bar(stat = \u0026#39;identity\u0026#39;, fill = \u0026quot;#4271AE\u0026quot;)+ coord_flip()+ labs(title=\u0026#39;Ethereum Sentiments\u0026#39;, x=\u0026#39;Sentiments\u0026#39;,y=\u0026#39;Sentiment Count\u0026#39;)+ theme_economist())  {\"x\":{\"data\":[{\"orientation\":\"h\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[1,3,4,4,6,6,6,8,15,25,26,27,92],\"y\":[1,2,3,4,5,6,7,8,9,10,11,12,13],\"text\":[\"reorder(as.character(sentiment), count): constraining\ncount: 1\",\"reorder(as.character(sentiment), count): litigious\ncount: 3\",\"reorder(as.character(sentiment), count): disgust\ncount: 4\",\"reorder(as.character(sentiment), count): uncertainty\ncount: 4\",\"reorder(as.character(sentiment), count): anger\ncount: 6\",\"reorder(as.character(sentiment), count): sadness\ncount: 6\",\"reorder(as.character(sentiment), count): surprise\ncount: 6\",\"reorder(as.character(sentiment), count): fear\ncount: 8\",\"reorder(as.character(sentiment), count): joy\ncount: 15\",\"reorder(as.character(sentiment), count): negative\ncount: 25\",\"reorder(as.character(sentiment), count): trust\ncount: 26\",\"reorder(as.character(sentiment), count): anticipation\ncount: 27\",\"reorder(as.character(sentiment), count): positive\ncount: 92\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(66,113,174,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":51.8655043586551,\"r\":13.2835201328352,\"b\":35.8655043586551,\"l\":99.626400996264},\"plot_bgcolor\":\"transparent\",\"paper_bgcolor\":\"rgba(213,228,235,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"title\":\" Ethereum Sentiments \",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":19.9252801992528},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4.6,96.6],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"25\",\"50\",\"75\"],\"tickvals\":[0,25,50,75],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"25\",\"50\",\"75\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(0,0,0,1)\",\"ticklen\":-6.6417600664176,\"tickwidth\":0.603796369674327,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.483037095739462,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Sentiment Count\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,13.6],\"tickmode\":\"array\",\"ticktext\":[\"constraining\",\"litigious\",\"disgust\",\"uncertainty\",\"anger\",\"sadness\",\"surprise\",\"fear\",\"joy\",\"negative\",\"trust\",\"anticipation\",\"positive\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13],\"categoryorder\":\"array\",\"categoryarray\":[\"constraining\",\"litigious\",\"disgust\",\"uncertainty\",\"anger\",\"sadness\",\"surprise\",\"fear\",\"joy\",\"negative\",\"trust\",\"anticipation\",\"positive\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":-6.6417600664176,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":1.05664364693007,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Sentiments\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"transparent\",\"bordercolor\":\"transparent\",\"borderwidth\":1.71796707229778,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":16.604400166044}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n // is this being viewed in RStudio?\\n if (location.search == '?viewer_pane=1') {\\n alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n } else {\\n window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n }\\n }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"17632edaf399\":{\"x\":{},\"y\":{},\"type\":\"bar\"}},\"cur_data\":\"17632edaf399\",\"visdat\":{\"17632edaf399\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]} Litecoin\nplotly::ggplotly(data_crypto_sentiment %\u0026gt;% filter(coin == \u0026quot;litecoin\u0026quot; \u0026amp; !is.na(sentiment)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) %\u0026gt;% ggplot(aes( x = reorder(as.character(sentiment), count), y = count))+ geom_bar(stat = \u0026#39;identity\u0026#39;, fill = \u0026quot;#4271AE\u0026quot;)+ coord_flip()+ labs(title=\u0026#39;Litecoin Sentiments\u0026#39;, x=\u0026#39;Sentiments\u0026#39;,y=\u0026#39;Sentiment Count\u0026#39;)+ theme_economist())  {\"x\":{\"data\":[{\"orientation\":\"h\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0],\"x\":[2,5,5,8,11,12,12,18,24,25,80],\"y\":[1,2,3,4,5,6,7,8,9,10,11],\"text\":[\"reorder(as.character(sentiment), count): uncertainty\ncount: 2\",\"reorder(as.character(sentiment), count): disgust\ncount: 5\",\"reorder(as.character(sentiment), count): sadness\ncount: 5\",\"reorder(as.character(sentiment), count): surprise\ncount: 8\",\"reorder(as.character(sentiment), count): anger\ncount: 11\",\"reorder(as.character(sentiment), count): fear\ncount: 12\",\"reorder(as.character(sentiment), count): joy\ncount: 12\",\"reorder(as.character(sentiment), count): trust\ncount: 18\",\"reorder(as.character(sentiment), count): anticipation\ncount: 24\",\"reorder(as.character(sentiment), count): negative\ncount: 25\",\"reorder(as.character(sentiment), count): positive\ncount: 80\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(66,113,174,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":51.8655043586551,\"r\":13.2835201328352,\"b\":35.8655043586551,\"l\":99.626400996264},\"plot_bgcolor\":\"transparent\",\"paper_bgcolor\":\"rgba(213,228,235,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"title\":\" Litecoin Sentiments \",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":19.9252801992528},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4,84],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"tickvals\":[0,20,40,60,80],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(0,0,0,1)\",\"ticklen\":-6.6417600664176,\"tickwidth\":0.603796369674327,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.483037095739462,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Sentiment Count\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,11.6],\"tickmode\":\"array\",\"ticktext\":[\"uncertainty\",\"disgust\",\"sadness\",\"surprise\",\"anger\",\"fear\",\"joy\",\"trust\",\"anticipation\",\"negative\",\"positive\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11],\"categoryorder\":\"array\",\"categoryarray\":[\"uncertainty\",\"disgust\",\"sadness\",\"surprise\",\"anger\",\"fear\",\"joy\",\"trust\",\"anticipation\",\"negative\",\"positive\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":-6.6417600664176,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":1.05664364693007,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Sentiments\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":13.2835201328352},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"transparent\",\"bordercolor\":\"transparent\",\"borderwidth\":1.71796707229778,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"sans\",\"size\":16.604400166044}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n // is this being viewed in RStudio?\\n if (location.search == '?viewer_pane=1') {\\n alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n } else {\\n window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n }\\n }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"1763560db35a\":{\"x\":{},\"y\":{},\"type\":\"bar\"}},\"cur_data\":\"1763560db35a\",\"visdat\":{\"1763560db35a\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}  ","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534723200,"objectID":"2bc8a9bee9d2601005946bbad58ed3d9","permalink":"/post/twitter_sentiment_analysis/","publishdate":"2018-08-20T00:00:00Z","relpermalink":"/post/twitter_sentiment_analysis/","section":"post","summary":"After performingn extensive data cleaning, I can now use the created web-scraping function that I built on my previous post to show the result of my sentiment analysis","tags":["R Markdown","Web-scraping Analysis","Sentiment Analysis"],"title":"Twitter Sentiment Analysis","type":"post"},{"authors":[],"categories":null,"content":" I performed a sentiment analysis on each individual cryptocurrency using Twitter API to have a better understanding about the influence of social media on cryptocurrencies.\nlibrary(twitteR) library(dplyr) library(tm) library(wordcloud) library(tidytext) library(tidyverse) library(sqldf) library(ggplot2) library(ggthemes) library(data.table) library(gridExtra)  Built-funcrion for Web Scraping Analysis I built a function to do some web-scraping analysis in order to facilitate the extraction of Twitter data.\nsetup_twitter_oauth(consumer_key, consumer_secret, access_token, access_token_secret) twitter_scraping_data \u0026lt;- function(coin_name){ coin_data = twitteR::searchTwitter(paste0(\u0026quot;#\u0026quot;,coin_name,\u0026quot; -filter:retweets\u0026quot;), lang = \u0026quot;en\u0026quot;, n = 100, since = '2017-08-01', until = as.character(as.Date(Sys.time())), retryOnRateLimit = 1) d = twitteR::twListToDF(coin_data) return(d) }  Loading the dataset from CoinMarket library(coinmarketcapr) all_coins \u0026lt;- get_marketcap_ticker_all() list_coin_names \u0026lt;- as.list(tolower(all_coins$name)) list_coin_names_20 \u0026lt;- list_coin_names[1:20]  Data Cleaning Part 1 datalist = list() for (i in list_coin_names_20) { data = twitter_scraping_data(i) data$coin_name = i datalist[[i]] = data } big_data = do.call(rbind, datalist ) big_data_text \u0026lt;- big_data %\u0026gt;% select(text, coin_name) big_data_text$text \u0026lt;- as.character(big_data_text$text) str(big_data_text) #applying to the big data datalist_v2 \u0026lt;- list() for (i in list_coin_names_20) { data = big_data_text %\u0026gt;% filter(coin_name == i) data$text = stripWhitespace(data$text) data$text = gsub(\u0026quot;[^[:alnum:][:space:]$]\u0026quot;, \u0026quot;\u0026quot;, data$text) data$text = tolower(data$text) data$text = removeWords(data$text, c(stopwords(\u0026quot;english\u0026quot;),'ampamp','retweet','just','comment','amp','bitcoin','btc','xrp','eth','crypto','cryptocurrency', paste0(i), all_coins$symbol)) data$coin_name = i datalist_v2[[i]] = data } big_data_clean = do.call(rbind, datalist_v2 )  Data Cleaning Part 2 datalist_v2 \u0026lt;- list() for ( i in list_coin_names_20 ) { data \u0026lt;- big_data_clean %\u0026gt;% filter(coin_name == i) %\u0026gt;% select(text) data$text \u0026lt;- as.character(data$text) datatweets = VectorSource(data$text) datatweets = VCorpus(datatweets) datatweets_dtm\u0026lt;-DocumentTermMatrix(datatweets) datatweets_m\u0026lt;-as.matrix(datatweets_dtm) datatweets_wf\u0026lt;-colSums(datatweets_m) datatweets_wf\u0026lt;-sort(datatweets_wf,decreasing = TRUE) datatweets_wf$coin \u0026lt;- i datalist_v2[[i]] = datatweets_wf } #More data cleaning data1 \u0026lt;- datalist_v2[[\u0026quot;bitcoin\u0026quot;]] data1 \u0026lt;- unlist(data1) data1 \u0026lt;- as.data.frame(data1) data1 \u0026lt;- cbind(words = rownames(data1), data1) rownames(data1) \u0026lt;- c() data1 \u0026lt;- data1 %\u0026gt;% mutate(words_count = data1) %\u0026gt;% select(words, words_count)  Data Cleaning Part 3 #combining everything now datalist_v3 \u0026lt;- list() for ( i in list_coin_names_20) { data1 \u0026lt;- datalist_v2[[i]] data1 \u0026lt;- unlist(data1) data1 \u0026lt;- as.data.frame(data1) data1 \u0026lt;- cbind(words = rownames(data1), data1) rownames(data1) \u0026lt;- c() data1 \u0026lt;- data1 %\u0026gt;% mutate(words_count = data1) %\u0026gt;% select(words, words_count) data1$coin \u0026lt;- i datalist_v3[[i]] = data1 } big_data_clean_2 = do.call(rbind, datalist_v3 ) big_data_clean_2$words_count \u0026lt;- as.numeric(big_data_clean_2$words_count) big_data_clean_2$words \u0026lt;- as.character(big_data_clean_2$words) write.csv(big_data_clean_2, \u0026quot;big_data_clean_2.csv\u0026quot;)  Data Cleaning Part 4 datalist_v4 \u0026lt;- list() for (i in list_coin_names_20) { data \u0026lt;- big_data_text %\u0026gt;% filter(coin_name == i) %\u0026gt;% select(text) data$text \u0026lt;- as.character(data$text) data \u0026lt;- data$text data \u0026lt;- as.tibble(data) data \u0026lt;- data %\u0026gt;% unnest_tokens(word, value) data$coin \u0026lt;- i datalist_v4[[i]] = data } big_data_clean_4 = do.call(rbind, datalist_v4 ) statement = paste0(\u0026quot; select sentiments.*,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word as SentimentWord from sentiments,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot; where sentiments.word = \u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word \u0026quot;) datalist_v5 \u0026lt;- list() for (i in list_coin_names_20) { DF \u0026lt;- big_data_clean_4 %\u0026gt;% filter(coin == i) %\u0026gt;% select(word) statement = paste0(\u0026quot; select sentiments.*,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word as SentimentWord from sentiments,\u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot; where sentiments.word = \u0026quot;,\u0026quot;DF\u0026quot;,\u0026quot;.word \u0026quot;) data \u0026lt;- sqldf(statement) data$coin \u0026lt;- i datalist_v5[[i]] = data } big_data_clean_5 = do.call(rbind, datalist_v5 ) data_v5 \u0026lt;- big_data_clean_5[!duplicated(big_data_clean_5), ] write.csv(data_v5, \u0026quot;data_crypto_sentiment.csv\u0026quot;) data_crypto_sentiment \u0026lt;- read.csv('data_crypto_sentiment.csv')  ","date":1532070000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534143600,"objectID":"b90faca0b8c52309503e75f801334405","permalink":"/post/web-scraping-twitter/","publishdate":"2018-07-20T00:00:00-07:00","relpermalink":"/post/web-scraping-twitter/","section":"post","summary":"Here is my web scraping analysis using Twitter API to perform sentiment analysis on cryptocurrency","tags":["Academic"],"title":"Web Scraping Analysis Function","type":"post"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530169200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530169200,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00-07:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":["R"],"content":"Loading Data set and libraries library(dplyr) library(ggplot2) library(stringr) library(caret) library(Hmisc) library(randomForest) library(readr) train \u0026lt;- read_csv(\u0026quot;../../static/data/train.csv\u0026quot;) test \u0026lt;- read_csv(\u0026quot;../../static/data/test.csv\u0026quot;)  Data exploration summary(test) ## PassengerId Pclass Name Sex ## Min. : 892.0 Min. :1.000 Length:418 Length:418 ## 1st Qu.: 996.2 1st Qu.:1.000 Class :character Class :character ## Median :1100.5 Median :3.000 Mode :character Mode :character ## Mean :1100.5 Mean :2.266 ## 3rd Qu.:1204.8 3rd Qu.:3.000 ## Max. :1309.0 Max. :3.000 ## ## Age SibSp Parch Ticket ## Min. : 0.17 Min. :0.0000 Min. :0.0000 Length:418 ## 1st Qu.:21.00 1st Qu.:0.0000 1st Qu.:0.0000 Class :character ## Median :27.00 Median :0.0000 Median :0.0000 Mode :character ## Mean :30.27 Mean :0.4474 Mean :0.3923 ## 3rd Qu.:39.00 3rd Qu.:1.0000 3rd Qu.:0.0000 ## Max. :76.00 Max. :8.0000 Max. :9.0000 ## NA\u0026#39;s :86 ## Fare Cabin Embarked ## Min. : 0.000 Length:418 Length:418 ## 1st Qu.: 7.896 Class :character Class :character ## Median : 14.454 Mode :character Mode :character ## Mean : 35.627 ## 3rd Qu.: 31.500 ## Max. :512.329 ## NA\u0026#39;s :1 summary(train) ## PassengerId Survived Pclass Name ## Min. : 1.0 Min. :0.0000 Min. :1.000 Length:891 ## 1st Qu.:223.5 1st Qu.:0.0000 1st Qu.:2.000 Class :character ## Median :446.0 Median :0.0000 Median :3.000 Mode :character ## Mean :446.0 Mean :0.3838 Mean :2.309 ## 3rd Qu.:668.5 3rd Qu.:1.0000 3rd Qu.:3.000 ## Max. :891.0 Max. :1.0000 Max. :3.000 ## ## Sex Age SibSp Parch ## Length:891 Min. : 0.42 Min. :0.000 Min. :0.0000 ## Class :character 1st Qu.:20.12 1st Qu.:0.000 1st Qu.:0.0000 ## Mode :character Median :28.00 Median :0.000 Median :0.0000 ## Mean :29.70 Mean :0.523 Mean :0.3816 ## 3rd Qu.:38.00 3rd Qu.:1.000 3rd Qu.:0.0000 ## Max. :80.00 Max. :8.000 Max. :6.0000 ## NA\u0026#39;s :177 ## Ticket Fare Cabin Embarked ## Length:891 Min. : 0.00 Length:891 Length:891 ## Class :character 1st Qu.: 7.91 Class :character Class :character ## Mode :character Median : 14.45 Mode :character Mode :character ## Mean : 32.20 ## 3rd Qu.: 31.00 ## Max. :512.33 ##  #We can see that survived is missing on the test data, let\u0026#39;s input it as a NA value for now test \u0026lt;- test %\u0026gt;% mutate(Survived = NA) #Lets merge the two dataset together for data wrangling/cleaning merge_data \u0026lt;- rbind(train, test) #Checking if there are no duplicates length(unique(merge_data$PassengerId)) == 891 + 418 ## [1] TRUE #TRUE Looking at the Survived by Sex\ntrain %\u0026gt;% ggplot(aes(x = Sex, fill = as.factor(Survived)))+ geom_bar()+ geom_text(stat = \u0026#39;count\u0026#39;, aes(label = ..count..))+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;) Looking at the Survived by Pclass\ntrain %\u0026gt;% ggplot(aes(x = as.factor(Pclass), fill = as.factor(Survived)))+ geom_bar()+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Pclass\u0026quot;) LOoking at the Survived by Sibsp\ntrain %\u0026gt;% ggplot(aes(x = as.factor(SibSp), fill = as.factor(Survived)))+ geom_bar()+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;SibSp\u0026quot;) Looking at the Survived by Sibsp and Sex\ntrain %\u0026gt;% ggplot(aes(x = as.factor(SibSp), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;SibSp\u0026quot;) Looking at the Survived by Pclass and Sex\ntrain %\u0026gt;% ggplot(aes(x = as.factor(Pclass), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Pclass\u0026quot;) Looking at the age distribution\nmerge_data %\u0026gt;% ggplot(aes(x = Age))+ geom_histogram(fill = \u0026quot;#000099\u0026quot;, color = \u0026quot;black\u0026quot;) Data understanding and Data cleaning Lets group the age into bucket\n#Lets convert some of the columns name into factors merge_data$Sex \u0026lt;- as.factor(merge_data$Sex) merge_data$Pclass \u0026lt;- as.factor(merge_data$Pclass) #First lets replace the missing values of age by the mean of the subgroup by pclass since it has no missing values merge_data %\u0026gt;% filter(!is.na(Age)) %\u0026gt;% group_by(Pclass) %\u0026gt;% summarise(Age_Pclass_mean = round(mean(Age))) ## # A tibble: 3 x 2 ## Pclass Age_Pclass_mean ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 39 ## 2 2 30 ## 3 3 25 # 1 39 # 2 30 # 3 25 #now lets replace the missing age values merge_data_missing_age \u0026lt;- merge_data %\u0026gt;% filter(is.na(Age)) %\u0026gt;% mutate(Age = ifelse(Pclass == \u0026quot;1\u0026quot;, 39 , ifelse(Pclass == \u0026quot;2\u0026quot;, 30, 25))) %\u0026gt;% select(PassengerId, Age) merge_data \u0026lt;- merge_data %\u0026gt;% left_join(merge_data_missing_age, by = \u0026#39;PassengerId\u0026#39;) %\u0026gt;% mutate(Age = ifelse(is.na(Age.x), Age.y, Age.x)) %\u0026gt;% select(-Age.x, -Age.y) merge_data$Age_bucket = ifelse(merge_data$Age \u0026lt; 12 , \u0026quot;\u0026lt;12\u0026quot;, ifelse(merge_data$Age \u0026gt;= 12 \u0026amp; merge_data$Age \u0026lt; 21, \u0026quot;16-21\u0026quot;, ifelse(merge_data$Age \u0026gt;= 21 \u0026amp; merge_data$Age \u0026lt; 30, \u0026quot;21-30\u0026quot;, ifelse(merge_data$Age \u0026gt;= 30 \u0026amp; merge_data$Age \u0026lt; 40, \u0026quot;30-40\u0026quot;, ifelse(merge_data$Age \u0026gt;= 40 \u0026amp; merge_data$Age \u0026lt; 50, \u0026quot;40-50\u0026quot;,\u0026quot;50+\u0026quot;))))) merge_data %\u0026gt;% group_by(Age_bucket) %\u0026gt;% tally() ## # A tibble: 6 x 2 ## Age_bucket n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 \u0026lt;12 91 ## 2 16-21 158 ## 3 21-30 528 ## 4 30-40 287 ## 5 40-50 135 ## 6 50+ 110 merge_data$Age_bucket \u0026lt;- as.factor(merge_data$Age_bucket) merge_data[1:891,] %\u0026gt;% ggplot(aes(x = as.factor(Age_bucket), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Age Bucket\u0026quot;) Lets extract the first letter from the Cabin\nlength(unique(merge_data$Cabin)) ## [1] 187 merge_data$Cabin_Letter \u0026lt;- str_sub(merge_data$Cabin,1,1) #creating a boolean variable merge_data$Cabin_Available \u0026lt;- as.factor(ifelse(is.na(merge_data$Cabin),FALSE,TRUE)) Fare\nmerge_data %\u0026gt;% ggplot(aes(x = Fare))+ geom_histogram(fill = \u0026quot;#000099\u0026quot;, color = \u0026quot;black\u0026quot;) sum(is.na(merge_data$Fare)) ## [1] 1 merge_data[which(is.na(merge_data$Fare)),] ## # A tibble: 1 x 15 ## PassengerId Survived Pclass Name Sex SibSp Parch Ticket Fare Cabin ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1044 NA 3 Storâ€¦ male 0 0 3701 NA \u0026lt;NA\u0026gt; ## # ... with 5 more variables: Embarked \u0026lt;chr\u0026gt;, Age \u0026lt;dbl\u0026gt;, Age_bucket \u0026lt;fct\u0026gt;, ## # Cabin_Letter \u0026lt;chr\u0026gt;, Cabin_Available \u0026lt;fct\u0026gt; #lets replace the fare by mean of Pclass = 3 merge_data[1044,]$Fare \u0026lt;- mean(subset(merge_data,Pclass==3)$Fare,na.rm=TRUE) merge_data$Fare_bucket \u0026lt;- cut2(merge_data$Fare,g=5) merge_data$Fare_bucket \u0026lt;- as.factor(merge_data$Fare_bucket) merge_data[1:891,] %\u0026gt;% ggplot(aes(x = as.factor(Fare_bucket), fill = as.factor(Survived)))+ geom_bar()+ facet_wrap(~Sex)+ scale_fill_discrete(name = \u0026#39;Survived\u0026#39;)+ xlab(\u0026quot;Fare Bucket\u0026quot;) Title\nmerge_data$Title \u0026lt;- merge_data$Title \u0026lt;- gsub(\u0026#39;(.*, )|(\\\\..*)\u0026#39;, \u0026#39;\u0026#39;, merge_data$Name) unique(merge_data$Title) ## [1] \u0026quot;Mr\u0026quot; \u0026quot;Mrs\u0026quot; \u0026quot;Miss\u0026quot; \u0026quot;Master\u0026quot; ## [5] \u0026quot;Don\u0026quot; \u0026quot;Rev\u0026quot; \u0026quot;Dr\u0026quot; \u0026quot;Mme\u0026quot; ## [9] \u0026quot;Ms\u0026quot; \u0026quot;Major\u0026quot; \u0026quot;Lady\u0026quot; \u0026quot;Sir\u0026quot; ## [13] \u0026quot;Mlle\u0026quot; \u0026quot;Col\u0026quot; \u0026quot;Capt\u0026quot; \u0026quot;the Countess\u0026quot; ## [17] \u0026quot;Jonkheer\u0026quot; \u0026quot;Dona\u0026quot; #We can see that there some titles that seem to have some luxurious power such as Sir, Major etc... Power_Title \u0026lt;- c(\u0026quot;Capt\u0026quot;,\u0026quot;Col\u0026quot;,\u0026quot;Don\u0026quot;,\u0026quot;Dona\u0026quot;,\u0026quot;Dr\u0026quot;,\u0026quot;Jonkheer\u0026quot;,\u0026quot;Lady\u0026quot;,\u0026quot;Major\u0026quot;, \u0026quot;Mlle\u0026quot;, \u0026quot;Mme\u0026quot;,\u0026quot;Rev\u0026quot;,\u0026quot;Sir\u0026quot;,\u0026quot;the Countess\u0026quot;) merge_data$Title[merge_data$Title %in% Power_Title] \u0026lt;- \u0026quot;Luxurious Title\u0026quot; merge_data$Title \u0026lt;- as.factor(merge_data$Title) Embarked\nmerge_data$Embarked %\u0026gt;% head() ## [1] \u0026quot;S\u0026quot; \u0026quot;C\u0026quot; \u0026quot;S\u0026quot; \u0026quot;S\u0026quot; \u0026quot;S\u0026quot; \u0026quot;Q\u0026quot; merge_data$Embarked \u0026lt;- as.factor(merge_data$Embarked) str(merge_data) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1309 obs. of 17 variables: ## $ PassengerId : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Survived : int 0 1 1 1 0 0 0 0 1 1 ... ## $ Pclass : Factor w/ 3 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;: 3 1 3 1 3 3 1 3 3 2 ... ## $ Name : chr \u0026quot;Braund, Mr. Owen Harris\u0026quot; \u0026quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)\u0026quot; \u0026quot;Heikkinen, Miss. Laina\u0026quot; \u0026quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)\u0026quot; ... ## $ Sex : Factor w/ 2 levels \u0026quot;female\u0026quot;,\u0026quot;male\u0026quot;: 2 1 1 1 2 2 2 2 1 1 ... ## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ... ## $ Ticket : chr \u0026quot;A/5 21171\u0026quot; \u0026quot;PC 17599\u0026quot; \u0026quot;STON/O2. 3101282\u0026quot; \u0026quot;113803\u0026quot; ... ## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ... ## $ Cabin : chr NA \u0026quot;C85\u0026quot; NA \u0026quot;C123\u0026quot; ... ## $ Embarked : Factor w/ 3 levels \u0026quot;C\u0026quot;,\u0026quot;Q\u0026quot;,\u0026quot;S\u0026quot;: 3 1 3 3 3 2 3 3 3 1 ... ## $ Age : num 22 38 26 35 35 25 54 2 27 14 ... ## $ Age_bucket : Factor w/ 6 levels \u0026quot;\u0026lt;12\u0026quot;,\u0026quot;16-21\u0026quot;,..: 3 4 3 4 4 3 6 1 3 2 ... ## $ Cabin_Letter : chr NA \u0026quot;C\u0026quot; NA \u0026quot;C\u0026quot; ... ## $ Cabin_Available: Factor w/ 2 levels \u0026quot;FALSE\u0026quot;,\u0026quot;TRUE\u0026quot;: 1 2 1 2 1 1 2 1 1 1 ... ## $ Fare_bucket : Factor w/ 5 levels \u0026quot;[ 0.00, 7.88)\u0026quot;,..: 1 5 2 5 2 2 5 3 3 4 ... ## $ Title : Factor w/ 6 levels \u0026quot;Luxurious Title\u0026quot;,..: 4 5 3 5 4 4 4 2 5 5 ... summary(merge_data$Embarked) ## C Q S NA\u0026#39;s ## 270 123 914 2 merge_data[which(is.na(merge_data$Embarked)),] ## # A tibble: 2 x 17 ## PassengerId Survived Pclass Name Sex SibSp Parch Ticket Fare Cabin ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 62 1 1 Icarâ€¦ femaâ€¦ 0 0 113572 80 B28 ## 2 830 1 1 Stonâ€¦ femaâ€¦ 0 0 113572 80 B28 ## # ... with 7 more variables: Embarked \u0026lt;fct\u0026gt;, Age \u0026lt;dbl\u0026gt;, Age_bucket \u0026lt;fct\u0026gt;, ## # Cabin_Letter \u0026lt;chr\u0026gt;, Cabin_Available \u0026lt;fct\u0026gt;, Fare_bucket \u0026lt;fct\u0026gt;, ## # Title \u0026lt;fct\u0026gt; merge_data[c(62,830),\u0026quot;Embarked\u0026quot;] \u0026lt;- \u0026quot;S\u0026quot; SibSp\nmerge_data$SibSp \u0026lt;- as.factor(merge_data$SibSp) Ticket\nlibrary(plyr) merge_data \u0026lt;- ddply(merge_data,.(Ticket),transform,Ticketsize=length(Ticket)) merge_data$Ticketsize \u0026lt;- as.factor(merge_data$Ticketsize) merge_data \u0026lt;- merge_data %\u0026gt;% arrange(PassengerId)   Data Modeling merge_data \u0026lt;- merge_data %\u0026gt;% select(Survived,Pclass,Sex,Age_bucket,Fare_bucket, Cabin_Available,Title,Embarked,Ticketsize,SibSp) #resplitting the merge data train_merge \u0026lt;- merge_data[1:891,] test_merge \u0026lt;- merge_data[892:1309,] train_merge$Survived \u0026lt;- as.factor(train_merge$Survived) #splitting the train merge dataset into a build and validate set set.seed(123) smp_size \u0026lt;- floor(0.75* nrow(train_merge)) train_ind \u0026lt;- sample(seq_len(nrow(train_merge)), size = smp_size) train \u0026lt;- train_merge[train_ind, ] test \u0026lt;- train_merge[-train_ind, ] train$Survived \u0026lt;- as.factor(train$Survived) Logistic Regression\nlog.fit \u0026lt;- glm(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + Ticketsize, family = binomial(link = logit), data = train) summary(log.fit) ## ## Call: ## glm(formula = Survived ~ Pclass + Sex + Age_bucket + Fare_bucket + ## Cabin_Available + Title + Embarked + Ticketsize, family = binomial(link = logit), ## data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6605 -0.5219 -0.3728 0.5767 2.3858 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 16.01336 1185.81297 0.014 0.989226 ## Pclass2 -0.47129 0.60763 -0.776 0.437969 ## Pclass3 -1.46573 0.71757 -2.043 0.041088 * ## Sexmale -17.00320 1185.81286 -0.014 0.988560 ## Age_bucket16-21 -0.39725 0.69179 -0.574 0.565808 ## Age_bucket21-30 -0.56230 0.65494 -0.859 0.390592 ## Age_bucket30-40 -0.74344 0.68532 -1.085 0.278012 ## Age_bucket40-50 -1.05592 0.75473 -1.399 0.161796 ## Age_bucket50+ -1.77229 0.79633 -2.226 0.026044 * ## Fare_bucket[ 7.88, 10.52) 0.33903 0.38161 0.888 0.374309 ## Fare_bucket[10.52, 22.02) 0.20834 0.44760 0.465 0.641612 ## Fare_bucket[22.02, 42.40) 0.40374 0.57457 0.703 0.482251 ## Fare_bucket[42.40,512.33] 0.97666 0.88614 1.102 0.270394 ## Cabin_AvailableTRUE 0.70778 0.42270 1.674 0.094044 . ## TitleMaster 3.83349 1.11306 3.444 0.000573 *** ## TitleMiss -13.24423 1185.81262 -0.011 0.991089 ## TitleMr 0.92542 0.83912 1.103 0.270092 ## TitleMrs -12.65381 1185.81265 -0.011 0.991486 ## TitleMs 1.91717 2676.55870 0.001 0.999428 ## EmbarkedQ 0.11511 0.48606 0.237 0.812795 ## EmbarkedS -0.53921 0.29872 -1.805 0.071065 . ## Ticketsize2 -0.51018 0.41490 -1.230 0.218829 ## Ticketsize3 -0.01865 0.52926 -0.035 0.971886 ## Ticketsize4 0.23315 0.69515 0.335 0.737325 ## Ticketsize5 -2.16015 0.86435 -2.499 0.012448 * ## Ticketsize6 -3.60736 0.96102 -3.754 0.000174 *** ## Ticketsize7 -2.11423 0.90612 -2.333 0.019634 * ## Ticketsize8 1.69420 1.16373 1.456 0.145436 ## Ticketsize11 -16.89587 761.09246 -0.022 0.982289 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 896.48 on 667 degrees of freedom ## Residual deviance: 541.04 on 639 degrees of freedom ## AIC: 599.04 ## ## Number of Fisher Scoring iterations: 15 log_prediction \u0026lt;- predict(log.fit, newdata = test, type = \u0026quot;response\u0026quot;) log_prediction \u0026lt;- ifelse(log_prediction \u0026gt; 0.5 , 1 , 0) logistic_classification_error \u0026lt;- mean(log_prediction != test$Survived) logistic_accuracy \u0026lt;- 1 - logistic_classification_error library(formattable) print(paste0(\u0026quot;The Accuracy for the Logistic Regression is: \u0026quot;, percent(logistic_accuracy))) ## [1] \u0026quot;The Accuracy for the Logistic Regression is: 83.86%\u0026quot;  Letâ€™s use the Caret package to improvde our predictive abilities by using 10-fold cross validation trainControl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3) metric \u0026lt;- \u0026quot;Accuracy\u0026quot; Logistic Regression\nset.seed(123) logistic.fit_2 \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket + Title + Embarked + SibSp, data = train, method = \u0026quot;glm\u0026quot;, metric = metric , trControl = trainControl) KNN\nlibrary(caret) set.seed(123) knn.fit \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;knn\u0026quot;, metric = metric , trControl = trainControl) SVM\nset.seed(123) svm.fit \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;svmRadial\u0026quot;, metric = metric , trControl = trainControl) ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. ## Warning in .local(x, ...): Variable(s) `\u0026#39; constant. Cannot scale data. Naives Bayes\nset.seed(123) naives.fit \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket +Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;nb\u0026quot;, metric = metric , trControl = trainControl) results \u0026lt;- resamples(list(Logistic_Regression =logistic.fit_2, KNN=knn.fit, NB=naives.fit, SVM=svm.fit)) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: Logistic_Regression, KNN, NB, SVM ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. ## Logistic_Regression 0.6417910 0.7735753 0.8030303 0.7979853 0.8302239 ## KNN 0.6865672 0.7611940 0.7985740 0.7939678 0.8308824 ## NB 0.6029412 0.6176471 0.6390773 0.6477776 0.6716418 ## SVM 0.6865672 0.7918130 0.8059701 0.8139669 0.8446970 ## Max. NA\u0026#39;s ## Logistic_Regression 0.8656716 0 ## KNN 0.8955224 0 ## NB 0.7761194 0 ## SVM 0.8955224 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. ## Logistic_Regression 0.2555556 0.53229349 0.58748131 0.5759262 0.6353576 ## KNN 0.3446670 0.49402964 0.57015499 0.5594179 0.6313138 ## NB 0.0000000 0.04432432 0.09214695 0.1289352 0.1948945 ## SVM 0.3366337 0.56057328 0.59431765 0.6024010 0.6599500 ## Max. NA\u0026#39;s ## Logistic_Regression 0.7163324 0 ## KNN 0.7752755 0 ## NB 0.4885496 0 ## SVM 0.7793696 0 dotplot(results) From the plot above, we can see that SVm has a the highest accuracy\n SVM Algorithm tuning grid \u0026lt;- expand.grid(.sigma=c(0.005, 0.01, 0.015, 0.02), .C=seq(1, 10, by=1)) set.seed(1234) svm.fit_2 \u0026lt;- train(Survived ~ Pclass + Sex + Age_bucket+ Fare_bucket + Cabin_Available + Title + Embarked + SibSp, data = train, method = \u0026quot;svmRadial\u0026quot;, metric = metric , trControl = trainControl, tuneGrid = grid) print(svm.fit_2) ## Support Vector Machines with Radial Basis Function Kernel ## ## 668 samples ## 8 predictor ## 2 classes: \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 601, 602, 602, 601, 602, 600, ... ## Resampling results across tuning parameters: ## ## sigma C Accuracy Kappa ## 0.005 1 0.8064433 0.5882477 ## 0.005 2 0.8084187 0.5920974 ## 0.005 3 0.8084187 0.5920974 ## 0.005 4 0.8069335 0.5891977 ## 0.005 5 0.8104240 0.5967106 ## 0.005 6 0.8084335 0.5929460 ## 0.005 7 0.8084335 0.5929460 ## 0.005 8 0.8084335 0.5929460 ## 0.005 9 0.8079433 0.5919960 ## 0.005 10 0.8074458 0.5907116 ## 0.010 1 0.8084187 0.5920974 ## 0.010 2 0.8079433 0.5919960 ## 0.010 3 0.8069556 0.5897778 ## 0.010 4 0.8109887 0.5973230 ## 0.010 5 0.8129269 0.6012344 ## 0.010 6 0.8104318 0.5953686 ## 0.010 7 0.8089763 0.5915943 ## 0.010 8 0.8059686 0.5842785 ## 0.010 9 0.8074913 0.5879227 ## 0.010 10 0.8029684 0.5782804 ## 0.015 1 0.8074383 0.5902136 ## 0.015 2 0.8099862 0.5952770 ## 0.015 3 0.8109145 0.5971580 ## 0.015 4 0.8074984 0.5879617 ## 0.015 5 0.8034957 0.5795312 ## 0.015 6 0.8019437 0.5764796 ## 0.015 7 0.8014387 0.5755482 ## 0.015 8 0.7999457 0.5732824 ## 0.015 9 0.8009633 0.5760061 ## 0.015 10 0.8009558 0.5762771 ## 0.020 1 0.8069483 0.5889880 ## 0.020 2 0.8104245 0.5963789 ## 0.020 3 0.8055083 0.5843859 ## 0.020 4 0.8024488 0.5777598 ## 0.020 5 0.8024485 0.5788391 ## 0.020 6 0.8019586 0.5782266 ## 0.020 7 0.8034810 0.5822925 ## 0.020 8 0.8044763 0.5842724 ## 0.020 9 0.8034662 0.5822480 ## 0.020 10 0.8039861 0.5835940 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.01 and C = 5. plot(svm.fit_2)  Random Forest rf.fit \u0026lt;- randomForest(factor(Survived) ~Pclass + Sex + Age_bucket + Fare_bucket +Cabin_Available + Title + Embarked + Ticketsize, data = train ,nodesize=20) print(rf.fit) ## ## Call: ## randomForest(formula = factor(Survived) ~ Pclass + Sex + Age_bucket + Fare_bucket + Cabin_Available + Title + Embarked + Ticketsize, data = train, nodesize = 20) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 18.41% ## Confusion matrix: ## 0 1 class.error ## 0 361 43 0.1064356 ## 1 80 184 0.3030303 Based on the summary, the random forest model gives us an accuracy of 80.1%\nplot(rf.fit) importance(rf.fit) ## MeanDecreaseGini ## Pclass 15.094161 ## Sex 41.581258 ## Age_bucket 11.234586 ## Fare_bucket 13.705522 ## Cabin_Available 11.176175 ## Title 47.243331 ## Embarked 6.103164 ## Ticketsize 17.908145 rf.fit$confusion ## 0 1 class.error ## 0 361 43 0.1064356 ## 1 80 184 0.3030303 Based on the random forest variable importance, lets only keep the following: Pclass + Sex + Age_bucket+ Fare_bucket Title + Ticketsize\nrf.fit2 \u0026lt;- randomForest(factor(Survived) ~Pclass + Sex + Fare_bucket + Cabin_Available + Ticketsize + Embarked + Title , data = train, nodesize=20) rf.fit2$confusion ## 0 1 class.error ## 0 365 39 0.09653465 ## 1 86 178 0.32575758  Conclusion: Logistic Regression is the winner best_model \u0026lt;- rf.fit prediction \u0026lt;- predict(best_model, test_merge) submission \u0026lt;- data.frame(PassengerId=names(prediction),Survived=prediction) submission %\u0026gt;% head() ## PassengerId Survived ## 892 892 0 ## 893 893 0 ## 894 894 0 ## 895 895 0 ## 896 896 1 ## 897 897 0 #write.csv(submission, file = \u0026quot;fourth_submission.csv\u0026quot;, row.names = FALSE)  ","date":1516406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516406400,"objectID":"80eb3262d1faa932863a84a67d59db1e","permalink":"/post/titanic_survival/","publishdate":"2018-01-20T00:00:00Z","relpermalink":"/post/titanic_survival/","section":"post","summary":"In this Kaggle Challenge, we apply the tools of machine learning to predict which passengers survived the tragedy.","tags":["R Markdown","Supervised Machine Learning","Data Analysis"],"title":"Titanic_Survival","type":"post"},{"authors":null,"categories":null,"content":" Description This report provides a detailed analysis of the â€˜applicationsâ€™ dataset using supervised fraud algorithms and machine learning statistical techniques. The programming tools used for Data Cleaning, Modeling, and Evaluation were Microsoft Excel, Tableau, R, along with mySQL through R. The original dataset contains unique records of more 90,000 product applications over the year 2016. Necessary feature analysis include:\n Data Understanding and Data Cleaning (dealing with frivolous values) Feature Engineering (creating expert Variables, performing feature selection) Data Modeling (running fraud detection models) Data Evaluation (comparing model results)  Project Goal Our objective for this project is to use a supervised fraud algorithm to build a fraud detection model that predicts product application fraud with high predictive accuracy. More precisely, we aim to find the top performing machine learning model that is able to predict the highest number of fraudulent cases with a low false positive rate. The report will present several classification machine learning techniques for fraud detection and their performances on our dataset, in complete detail such as Logistic Regression, Random Forests, Support Vector Machine, and Gradient Boosting Trees.\nKey Findings Here are the main key findings from building a Fraud Detection Model on the Applications dataset:\n The Gradient Boosting Trees method yielded the best Fraud Detection model at 10% penetration on the OOT (Out of Time) dataset with a FDR (Fraud Detection Rate) of 13.02% including a FDR of 12.50% on the testing dataset The Support Vector Machine Method yielded the lowest Fraud Detection Rate (FDR) at 10% penetration on the OOT (Out of Time) dataset with a FDR (Fraud Detection Rate) of 11.81% including a FDR of 11.01% on the testing dataset Overall, Gradient Boosting Trees and Random Forest Models both demonstrated relatively high predictive performance, with the highest predictive power in their respective segments  Click to Download the full report\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"86243c7cfa5442accb0649241afb8458","permalink":"/project/supervised-learning/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/supervised-learning/","section":"project","summary":"Fraud Analytics: Supervised Learning and Fraud Detection in Product Applications.","tags":["Supervised Machine Learning"],"title":"Fraud Analytics: Fraud Detection Model","type":"project"},{"authors":null,"categories":null,"content":" Description This report provides a detailed analysis of â€˜New York City Property Tax Valuation Dataâ€™ and also aims at identifying potential fraudulent records from the data set using unsupervised machine learning methods. The programming tools used for Data Cleaning were performed on Microsoft Excel and R. The original dataset contains unique records of more than 1 million properties across the state of New York with 30 different fields, both numerical and categorical. Necessary feature analysis include:\n Data Understanding and Data Cleaning (estimating missing values of each fields) Data Modeling (creating expert variables) and Data Evaluation Fraud Score Calculation (Heuristic Algorithm and Autoencoder)  Project Goal Our objective for this project is to use machine learning algorithms such as Autoencoder and Heuristic Algorithms and calculate a fraud score for each of the New York Property data records to analyze anomalies for potential fraud detection. By applying both of the unsupervised machine learning methods, records with high scores show to be potentially fraudulent. The report will explain each step in complete detail.\nKey Findings The two fraud detection algorithms we used, Heuristic and Autoencoder-Based Anomaly Detection, had a considerably high overlap matching percentage among the top 1% of all records:\n Missing values were properly filled using reasonable data cleaning methodology. The 51 expert variables were carefully crafted to perform PCA analysis. After normalizing the fraud scores and visualizing the scores, we found both score distributions to be right skewed. Among the top 10 highest fraud score records, anomalies were found in several fields.  Click to Download the full report\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"/project/deep-learning/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Fraud Analytics: Unsupervised Learning and Fraud Score Calculation on New York property tax.","tags":["Unsupervised Machine Learning"],"title":"Fraud Analytics: Fraud Score Calculation","type":"project"},{"authors":null,"categories":null,"content":" Introduction Homelessness is soaring in Los Angeles and is a severe issue that is a part of everyday life. Our objective for this project:\n Determining the need, demand and potential effects of different strategies for homeless intervention. Identifying the areas with high-concentrations of unsheltered homelessness, crime incidents and high danger areas for the homeless community in order to help address the homelessness in full scope, while prioritizing service delivery needs  In order to gain insight on homelessness:\n We cleaned the data and used geospatial analysis to create an online mapping application. This online mapping tool helped us evaluate the effectiveness of existing and potential homelessness interventions, functioned as a framework to determine fast delivery alternatives as well as helping us analyze data in full-scope and allow the rapid deployment of resources and services.  Shiny Application The Interactive Map displays the city of Los Angeles and its homeless population in an interactive map. The collapsable Map Controls drop down has the following features:\n Change the homeless measure distribution by either Census Tract or Communities Filter the homeless population by year and measure Filter the Crime counts and locations by time of day, day of the week, and month of the year Filter the Call counts and locations by time of day, day of the week, and month of the year Note that the 24 hour day has been split into 4 buckets:  Morning: 12:00 AM to 6:00 AM Afternoon: 6:00 AM to 12:00 PM Evening: 12:00 PM to 6:00 PM Night: 6:00 PM to 12:00 AM  As well, above the legend in the bottom right corner, the user can select the different layers to display: Homeless Measure, Shelters, Crimes, and/or 311 calls.  The Crimes and Calls tab displays a plot of the distribution of 311 calls or crime counts across the week and then also buckets them into the time of day following the above mentioned splits. Crimes can be further filtered based on the type of crime from the drop down menu on the left. The Homeless Comparison tab displays the 2017 and 2016 homeless measures side by side to give a quick visual clue into the year over year homeless distribution change across the city of Los Angeles. Similar to the interactive map, the plot can be changed between viewing census tracts or communities. In addition, the same homeless measure filters can be applied, such as viewing the Total Homeless or Total Unsheltered distributions.\nConclusion and Key Findings  We analyzed and visualized the trend of homelessness and associated crimes with the merged dataset by census tract: We then Calculated the correlation between the homeless density and 311 calls  We observed a high correlation between density and crimes counts which raised the question that whether high crime rates were a threat to the homeless population or were the homeless causing these crimes  Analyzed the distribution of crimes and 311 calls across time\n Number of 311 calls was the highest on Tuesday and during the afternoon hours Crime rate is highest on Saturday afternoon, followed by Tuesday evening and Sunday afternoon and evening. The most frequent type of crime is Assault on Saturday afternoon   Used our Shiny Dashboard to understand which factors could be significant for the unsheltered homeless. Skid row being at the top even though it has the largest number of shelter counts. Worth noting that calls coming from that area are comparatively quite low and it experiences high crime counts as well.\n  Recommendations:  Investigate and access to homeless satisfaction in shelters Analyze average age of homeless in shelters to estimate the average age of the total homeless population per census tract. By doing so, we can have then use this data to implement better strategies and procedures to help homelessness. Collect more data from successful project to support the decreasing rate of unsheltered homeless.  Click to Download the full presentation\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"72fd2265f356f4cda4a73c57de4fd5bd","permalink":"/project/homeless_project/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/homeless_project/","section":"project","summary":"This online geospatial visualization mapping tool helped us evaluate the effectiveness of existing and potential homelessness interventions.","tags":["Geospatial Visualization"],"title":"Homeless Geospatial Visualization Dashboard with Shiny","type":"project"},{"authors":null,"categories":null,"content":" Executive Summary For several universities across the world, the implementation of a systematic approach to course scheduling remains a problem. Discrete optimization approaches have been used to solve the problem independently at such institutions, however owing to the complex nature of the problem compounded by the viewpoints of various stakeholders, a universal â€œcookie-cutterâ€ solution does not exist. There are 7 departments in the Marshall School of Business at USC, and course scheduling is currently performed by reusing the course schedule from the previous year as closely as possible and manually implementing a series of reforms to improve efficiency and transparency. The current process assigns over 400 courses into 45 schedulable rooms at 42 different times. However, the schedules produced by the current process are not always transparent and efficient. Additionally, sections are scheduled and de-scheduled multiple times during the course allotment process which makes greedy approaches to the allocation problem inefficient.\nOur solution to the problem involves the creation of a tractable MIP-based allocation tool that equitably assigns departments to classroom-timeslots. An algorithm developed in-house generates classroom-timeslot preference scores for each department based on survey results, and every run of the allocation tool outputs candidate sets of classroom-timeslots for each department. The size of the candidate sets is dependent upon a model-tuning parameter that controls the greediness of the allocation approach. This grants a greater degree of flexibility in course-scheduling and ensures that a certain quantity of classroom-timeslots of each category are available for the next (potential) run of the allocation tool to assign the remaining sections. The focus of this report is to describe the functioning of the tool, and the business value it offers to the administrative wing of the Marshall School of Business.\nIn the first step of the report, we propose a precise and high-level explanation on how the application of Mixed Integer Programming can be used to solve the course scheduling problem. In the second step of the report, we formulate a Mixed Integer Program by creating the input and output data, the decision variables, the objective function and the constraints. We conclude with a description of the output from one run of the model, and why the model makes sense and is easily implementable.\nClick to Download the full report\n","date":1461740400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461740400,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"/project/example-external-project/","publishdate":"2016-04-27T00:00:00-07:00","relpermalink":"/project/example-external-project/","section":"project","summary":"A MIP-based Approach to Optimizing Course Scheduling at USC Marshall","tags":["Optimization"],"title":"Optimizing Course Scheduling","type":"project"}]